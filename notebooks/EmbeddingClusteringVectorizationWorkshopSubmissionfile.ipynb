{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings, Clustering and Vectorization Workshop\n",
    "Tutorial for extracting word embeddings from words.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Vector Stores and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî§ Introduction to Word2Vec\n",
    "\n",
    "**What is Word2Vec?**\n",
    "\n",
    "Word2Vec is a popular algorithm used in Natural Language Processing (NLP) to transform words into vector representations.\n",
    "It learns these word vectors (embeddings) from a large collection of text so that words with similar meanings are located close to each other in a high-dimensional space.\n",
    "\n",
    "**Who developed it and when?**\n",
    "\n",
    "Word2Vec was developed by a team of researchers at Google, led by Tomas Mikolov, in 2013.\n",
    "\n",
    "**Who currently maintains and supports new releases?**\n",
    "\n",
    "While the original research came from Google, the open-source Python library gensim now maintains Word2Vec functionality.\n",
    "gensim is maintained by the open-source community, originally developed by Radim ≈òeh≈Ø≈ôek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî§ Word2Vec embeddings using the Gensim library\n",
    "\n",
    "Word2Vec is a popular technique for learning word embeddings, which are dense vector representations of words that capture semantic relationships between words based on their context.<br>\n",
    "As discussed, Word2Vec have 2 types, Skipgrams and CBOW. Where SkipGrams are trained to predict context words given the target word, however CBOW is trained to predict target words given its context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Requirements: downloading punkt from nltk, and installing gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'punkt' tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to c:\\Users\\mathe\\Downloads\\Embe\n",
      "[nltk_data]     ddingClusteringVectorizationWorkshop\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to c:\\Users\\mathe\\Downloads\\\n",
      "[nltk_data]     EmbeddingClusteringVectorizationWorkshop\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'punkt_tab' tokenizer...\n",
      "NLTK Data Paths: ['C:\\\\Users\\\\mathe/nltk_data', 'c:\\\\Users\\\\mathe\\\\Downloads\\\\EmbeddingClusteringVectorizationWorkshop\\\\.venv\\\\nltk_data', 'c:\\\\Users\\\\mathe\\\\Downloads\\\\EmbeddingClusteringVectorizationWorkshop\\\\.venv\\\\share\\\\nltk_data', 'c:\\\\Users\\\\mathe\\\\Downloads\\\\EmbeddingClusteringVectorizationWorkshop\\\\.venv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\mathe\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'c:\\\\Users\\\\mathe\\\\Downloads\\\\EmbeddingClusteringVectorizationWorkshop\\\\nltk_data']\n",
      "Contents of nltk_data: ['corpora', 'tokenizers']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# importing needed libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Warning: This download will copy files to your home directory.\n",
    "# For example, on Linux, it will copy files to ~/.nltk_data.\n",
    "# In Windows, it will copy files to C:\\Users\\YourAccount\\AppData\\Roaming\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# A better way to handle the download is to:\n",
    "# Ensure 'punkt' is available and nltk_data path is set\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "print(\"Downloading 'punkt' tokenizer...\")\n",
    "nltk.download('punkt', download_dir=nltk_data_path, force=True)\n",
    "print(\"Downloading 'punkt_tab' tokenizer...\")\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_path, force=True)\n",
    "\n",
    "# Always append the custom nltk_data path (if not already present)\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Debugging paths and contents\n",
    "print(\"NLTK Data Paths:\", nltk.data.path)\n",
    "print(\"Contents of nltk_data:\", os.listdir(nltk_data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do I use Word2Vec in Python and Jupyter Notebooks?**\n",
    "\n",
    "To use Word2Vec in Python, we typically start by following the NLP pipeline to produce a set of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Sample Sentence\n",
    "# text = \"\"\"\n",
    "# Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. NLP techniques aim to enable computers to understand, interpret, and generate human language in a way that is both meaningful and contextually relevant.\n",
    "# \"\"\"\n",
    "# tokenized_words = word_tokenize(text.lower())\n",
    "\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Setup\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the custom corpus from the file\n",
    "with open(\"./Data/corpus.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_corpus = file.read()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [w for w in tokens if w not in stopwords.words(\"english\")]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Preprocess and preview tokens\n",
    "tokenized_words = preprocess(raw_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a model in Word2Vec?**\n",
    "\n",
    "A Word2Vec model is a trained neural network that maps words from your dataset to numerical vectors (embeddings).\n",
    "The model learns word relationships based on how often they appear together in context.\n",
    "\n",
    "After the model is trained, each word in your vocabulary is now represented by a vector of numbers that can be used in tasks like similarity, clustering, or classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß† Explanation of the model creation instruction\n",
    "\n",
    "- `sentences`: A list of tokenized sentences (in this case, [tokenized_words] means a list of words).\n",
    "- `vector_size=100`: Each word will be represented by a vector with 100 dimensions.\n",
    "- `window=5`: The model considers 5 words before and after the target word (context window).\n",
    "- `min_count=1`: Include all words that appear at least once.\n",
    "- `workers=4`: Uses 4 CPU threads to speed up training.\n",
    "\n",
    "**This model will learn how words relate to each other and store them as vectors in its internal memory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a model\n",
    "model_word2Vec = Word2Vec(sentences=[tokenized_words], vector_size=100, window=5,  min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for 'natural' using Word2Vec: [ 9.0618785e-03  1.3109152e-03 -4.4864449e-03  6.4336923e-03\n",
      "  5.5817906e-03  5.7150479e-03 -4.1356703e-04  5.7349759e-03\n",
      " -4.5927783e-04  3.0258710e-03  4.1768989e-03 -3.1018970e-03\n",
      " -4.9922140e-03 -9.9668335e-03  2.3343451e-03  7.7250255e-03\n",
      "  7.9850052e-03  4.2921691e-03  1.3931717e-03  8.4922640e-03\n",
      " -8.0199149e-03  4.2265714e-03 -7.8323539e-03 -9.8017603e-03\n",
      " -8.2259270e-04 -7.6229712e-03 -9.4469503e-04 -6.5323366e-03\n",
      " -3.6708065e-03 -3.0943428e-03  5.3616650e-03 -2.3662109e-05\n",
      "  3.6458909e-03 -5.3184694e-03  4.2404146e-03  1.0042579e-02\n",
      "  5.2371481e-03 -9.4974153e-03 -8.0696549e-03 -1.5077193e-03\n",
      "  9.7858896e-03 -2.1861047e-03  6.5044356e-03  5.0952681e-03\n",
      "  1.0121750e-03 -1.0144808e-03 -2.4439618e-03 -8.0500739e-03\n",
      "  8.7374058e-03 -3.1807481e-03 -3.4151257e-03  3.5743972e-03\n",
      "  2.1323068e-03  7.2557558e-03 -6.1991960e-03  7.9208398e-03\n",
      " -8.2101058e-03 -4.7035711e-03 -7.2970870e-03  7.8268610e-03\n",
      "  2.0979422e-03  8.4781805e-03  7.5543192e-03 -8.5375673e-04\n",
      " -8.8499179e-03 -1.4128673e-03 -6.5934137e-03 -2.2714592e-03\n",
      "  2.3427750e-04  3.4364751e-03 -7.3287860e-03  9.6001904e-03\n",
      "  8.8380994e-03  4.1897218e-03  9.5638279e-03  2.9917507e-04\n",
      "  6.9591487e-03 -4.1351574e-03 -1.1729680e-03  8.8872723e-03\n",
      " -3.2486259e-03  2.2239869e-03  6.5045580e-03  7.1363240e-03\n",
      " -9.5791239e-03  1.0241633e-03 -5.1139728e-03 -4.5096753e-03\n",
      " -2.7590250e-03  6.0214042e-03 -4.9296753e-03  9.5153563e-03\n",
      " -4.4197123e-03 -2.7079799e-03  3.6639443e-03  5.7544746e-03\n",
      " -9.0117874e-03 -2.4865605e-03  5.5538560e-03  5.9972829e-03]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get word vector for a specific word\n",
    "word = \"natural\"\n",
    "vector_word2Vec = model_word2Vec.wv[word]\n",
    "print(f\"Word vector for '{word}' using Word2Vec: {vector_word2Vec}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### üìä Understanding Word Vectors in Word2Vec\n",
    "\n",
    "#### üî¢ Code Recap\n",
    "\n",
    "```python\n",
    "# Get word vector for a specific word\n",
    "word = \"natural\"\n",
    "vector_word2Vec = model_word2Vec.wv[word]\n",
    "print(f\"Word vector for '{word}' using Word2Vec: {vector_word2Vec}\")\n",
    "```\n",
    "\n",
    "This code retrieves the **vector representation (embedding)** of the word `\"natural\"` from the trained Word2Vec model.\n",
    "\n",
    "#### üß† What Is a Word Vector?\n",
    "\n",
    "In **Word2Vec**, every word in your vocabulary is represented by a **dense vector** of real numbers.\n",
    "\n",
    "* Each word becomes a **point** in a high-dimensional space.\n",
    "* Words that appear in similar contexts are placed **closer together**.\n",
    "* These vectors are learned by a shallow neural network during training.\n",
    "\n",
    "The output you see is a **vector with 100 dimensions**, because we set `vector_size=100` when training the model.\n",
    "\n",
    "#### ‚úçÔ∏è What Do These Numbers Mean?\n",
    "\n",
    "The output:\n",
    "\n",
    "```python\n",
    "[ 9.7677782e-03, 8.1660571e-03, ..., -2.3143364e-03 ]\n",
    "```\n",
    "\n",
    "...is a list of 100 floating-point numbers, like:\n",
    "\n",
    "$$\n",
    "\\vec{v}_{\\text{natural}} = [v_1, v_2, v_3, \\ldots, v_{100}]\n",
    "$$\n",
    "\n",
    "This vector encodes the **semantic meaning** of the word *natural* based on its context in the training data.\n",
    "\n",
    "While **each individual number doesn't mean anything by itself**, together they represent a **position** in a 100-dimensional space.\n",
    "\n",
    "#### üîç Why Is This Useful?\n",
    "\n",
    "You can perform various mathematical operations with these vectors:\n",
    "\n",
    "#### **Similarity** between words\n",
    "\n",
    "We can compute **cosine similarity** to check how similar two words are:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(\\vec{v}_a, \\vec{v}_b) = \\frac{\\vec{v}_a \\cdot \\vec{v}_b}{\\|\\vec{v}_a\\| \\|\\vec{v}_b\\|}\n",
    "$$\n",
    "\n",
    "If two word vectors point in a similar direction (small angle), they are semantically similar.\n",
    "\n",
    "#### üßÆ Summary\n",
    "\n",
    "* The output is a **100-dimensional vector** for the word `\"natural\"`.\n",
    "* Each dimension is a learned number that helps position the word in a semantic space.\n",
    "* These vectors are powerful tools for **clustering**, **search**, **recommendations**, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'natural' using Word2Vec: [('act', 0.3354310393333435), ('people', 0.2780853807926178), ('list', 0.2668672204017639), ('r', 0.24674904346466064), ('fire', 0.23556874692440033), ('receive', 0.23384404182434082), ('site', 0.22693923115730286), ('class', 0.21940377354621887), ('sea', 0.21812349557876587), ('prediction', 0.21565358340740204)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find similar words\n",
    "similar_words_word2Vec = model_word2Vec.wv.most_similar(word)\n",
    "print(f\"Similar words to '{word}' using Word2Vec: {similar_words_word2Vec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîç Finding Similar Words with Word2Vec\n",
    "\n",
    "#### üî¢ Code Recap\n",
    "\n",
    "```python\n",
    "# Find similar words\n",
    "similar_words_word2Vec = model_word2Vec.wv.most_similar(word)\n",
    "print(f\"Similar words to '{word}' using Word2Vec: {similar_words_word2Vec}\")\n",
    "```\n",
    "\n",
    "This code retrieves the **top 10 words** that are most similar to the word `\"natural\"` according to the Word2Vec model.\n",
    "\n",
    "#### üß† What Does ‚ÄúSimilar‚Äù Mean in Word2Vec?\n",
    "\n",
    "Word2Vec considers words to be similar if their **vector representations** are **close together** in high-dimensional space.\n",
    "\n",
    "This closeness is measured using **cosine similarity**:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(\\vec{v}_a, \\vec{v}_b) = \\frac{\\vec{v}_a \\cdot \\vec{v}_b}{\\|\\vec{v}_a\\| \\|\\vec{v}_b\\|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\vec{v}_a$ and $\\vec{v}_b$ are the word vectors.\n",
    "* $\\cdot$ is the dot product.\n",
    "* $\\|\\vec{v}\\|$ is the magnitude (length) of vector $\\vec{v}$.\n",
    "\n",
    "This produces a value between **-1 and 1**:\n",
    "\n",
    "* **1** ‚Üí perfectly similar (same direction)\n",
    "* **0** ‚Üí no similarity (orthogonal)\n",
    "* **-1** ‚Üí completely opposite\n",
    "\n",
    "#### üì§ Example Output Explained\n",
    "\n",
    "```python\n",
    "[('the', 0.182), ('computers', 0.173), ('nlp', 0.167), ('between', 0.156), ...]\n",
    "```\n",
    "\n",
    "This means:\n",
    "\n",
    "| Word       | Cosine Similarity |\n",
    "| ---------- | ----------------: |\n",
    "| the        |             0.182 |\n",
    "| computers  |             0.173 |\n",
    "| nlp        |             0.167 |\n",
    "| between    |             0.156 |\n",
    "| way        |             0.133 |\n",
    "| techniques |             0.122 |\n",
    "| human      |             0.112 |\n",
    "| is         |             0.111 |\n",
    "| .          |             0.109 |\n",
    "| in         |             0.097 |\n",
    "\n",
    "These words appeared in **similar contexts** to `\"natural\"` in the training data.\n",
    "\n",
    "üí° Example: If the training sentences included phrases like *\"natural language processing\"*, and also had *\"nlp techniques\"*, *\"human language\"*, or *\"way computers process language\"*, then `\"nlp\"`, `\"human\"`, `\"techniques\"`, and `\"computers\"` would naturally be nearby in vector space.\n",
    "\n",
    "#### üéØ Summary\n",
    "\n",
    "* Word2Vec learned that these words tend to appear **in similar contexts** to `\"natural\"`.\n",
    "* The similarity is calculated using **cosine similarity** on the word vectors.\n",
    "* The output is a **ranked list** of the most contextually similar words.\n",
    "\n",
    "You can use this method for:\n",
    "\n",
    "* **Synonym detection**\n",
    "* **Query expansion in search engines**\n",
    "* **Exploring semantic relationships in text**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKIPGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for 'natural' using Word2Vec: [ 8.7082842e-03  1.7755146e-03 -4.6348916e-03  6.5599727e-03\n",
      "  5.6444942e-03  5.0105508e-03 -2.7312606e-04  6.4833830e-03\n",
      " -9.4066962e-04  2.9214562e-03  4.3337266e-03 -3.7483610e-03\n",
      " -5.3374018e-03 -9.9668456e-03  2.4176307e-03  7.2421608e-03\n",
      "  8.2010692e-03  3.7001187e-03  1.0596707e-03  7.5370288e-03\n",
      " -7.9315165e-03  4.5400974e-03 -7.6508992e-03 -9.7950278e-03\n",
      " -1.1503731e-03 -7.6019932e-03 -1.1309383e-03 -7.3093167e-03\n",
      " -4.1056070e-03 -2.8169467e-03  5.8785067e-03 -5.3821721e-05\n",
      "  3.9895684e-03 -5.6087938e-03  3.8825977e-03  1.0963921e-02\n",
      "  5.0567808e-03 -1.0026055e-02 -8.4523065e-03 -2.8571649e-03\n",
      "  1.0209752e-02 -2.6833499e-03  6.4922743e-03  4.7351327e-03\n",
      "  1.6957803e-03 -1.2224368e-03 -2.4357368e-03 -8.3947126e-03\n",
      "  8.9911530e-03 -3.1165122e-03 -2.9638212e-03  3.0775752e-03\n",
      "  2.0849605e-03  7.3777065e-03 -6.5401667e-03  8.2778679e-03\n",
      " -8.0402652e-03 -4.6051149e-03 -7.8497641e-03  8.1806574e-03\n",
      "  2.2549282e-03  8.5326768e-03  7.8046764e-03 -1.4113381e-03\n",
      " -9.6934438e-03 -1.3495894e-03 -6.0910578e-03 -1.6711877e-03\n",
      " -6.5586501e-04  4.3613124e-03 -7.6320646e-03  9.5846197e-03\n",
      "  9.5250206e-03  4.4953297e-03  9.8914597e-03  6.6040910e-04\n",
      "  6.8984190e-03 -4.6677548e-03 -1.9121984e-03  9.1976272e-03\n",
      " -3.4158975e-03  1.7840422e-03  5.8217058e-03  7.8646261e-03\n",
      " -9.8534413e-03  1.1078662e-03 -5.2457177e-03 -3.9295130e-03\n",
      " -2.0696719e-03  6.1499146e-03 -4.2546047e-03  9.8982025e-03\n",
      " -4.8293704e-03 -2.3429263e-03  4.6541146e-03  6.2016277e-03\n",
      " -8.8490229e-03 -3.0124469e-03  6.0208593e-03  5.7461560e-03]\n",
      "Similar words to 'natural' using Word2Vec: [('act', 0.35471946001052856), ('people', 0.3178861141204834), ('list', 0.3019316792488098), ('prediction', 0.26579782366752625), ('r', 0.2625595033168793), ('site', 0.2625119686126709), ('fire', 0.2599733769893646), ('receive', 0.2525102198123932), ('class', 0.23461277782917023), ('let', 0.22721901535987854)]\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "model_skipGram = Word2Vec(sentences=[tokenized_words], vector_size=100, window=5,  min_count=1, workers=4, sg=1)\n",
    "\n",
    "# Get word vector for a specific word\n",
    "vector_skipGram = model_skipGram.wv[word]\n",
    "print(f\"Word vector for '{word}' using Word2Vec: {vector_skipGram}\")\n",
    "\n",
    "# Find similar words\n",
    "similar_words_skipGrams = model_skipGram.wv.most_similar(word)\n",
    "print(f\"Similar words to '{word}' using Word2Vec: {similar_words_skipGrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß† Understanding Skip-grams in Word2Vec\n",
    "\n",
    "### üîÅ What Are Skip-grams?\n",
    "\n",
    "A **Skip-gram** is a training approach used in Word2Vec that **predicts context words given a center word**.\n",
    "\n",
    "> üîë Goal: For a given target word, predict the words that are likely to appear nearby in a sentence.\n",
    "\n",
    "#### üìö How Does Skip-gram Work?\n",
    "\n",
    "Let‚Äôs take a sentence:\n",
    "\n",
    "```\n",
    "\"The field of natural language processing is growing.\"\n",
    "```\n",
    "\n",
    "If the center word is `\"natural\"` and the window size is 2, the context window is:\n",
    "\n",
    "```python\n",
    "[\"of\", \"natural\", \"language\", \"processing\"]\n",
    "```\n",
    "\n",
    "The skip-gram model will create training pairs like:\n",
    "\n",
    "```python\n",
    "(\"natural\", \"of\"), (\"natural\", \"language\")\n",
    "```\n",
    "<br/>\n",
    "\n",
    "üß† The model **learns embeddings** by trying to **maximize the probability** of seeing context words given the center word.\n",
    "\n",
    "#### üìê The Skip-gram Objective Function\n",
    "\n",
    "The skip-gram model aims to maximize the following log-likelihood objective over a large corpus:\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log P(w_{t+j} \\mid w_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $T$ is the total number of words in the corpus\n",
    "* $w_t$ is the center word\n",
    "* $w_{t+j}$ are the surrounding context words\n",
    "* $c$ is the window size\n",
    "\n",
    "The probability $P(w_{t+j} \\mid w_t)$ is computed using softmax:\n",
    "\n",
    "$$\n",
    "P(w_O \\mid w_I) = \\frac{\\exp\\left({\\vec{v}_{w_O}^\\top \\vec{v}_{w_I}}\\right)}{\\sum_{w = 1}^{V} \\exp\\left({\\vec{v}_w^\\top \\vec{v}_{w_I}}\\right)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\vec{v}_{w_I}$: vector of the input word (center)\n",
    "* $\\vec{v}_{w_O}$: vector of the output/context word\n",
    "* $V$: vocabulary size\n",
    "\n",
    "### üß™ Code Recap\n",
    "\n",
    "```python\n",
    "model_skipGram = Word2Vec(sentences=[tokenized_words], vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "```\n",
    "\n",
    "* `sg=1` activates the **Skip-gram** architecture (`sg=0` would activate CBOW instead).\n",
    "* This model now learns word embeddings by training on `(center ‚Üí context)` word pairs.\n",
    "\n",
    "#### üì§ Output Explanation\n",
    "\n",
    "#### ‚úÖ Word Vector\n",
    "\n",
    "```python\n",
    "vector_skipGram = model_skipGram.wv[word]\n",
    "```\n",
    "\n",
    "Returns a **100-dimensional vector** for the word `\"natural\"` that reflects its learned representation based on surrounding context words.\n",
    "\n",
    "#### ‚úÖ Most Similar Words\n",
    "\n",
    "```python\n",
    "model_skipGram.wv.most_similar(\"natural\")\n",
    "```\n",
    "\n",
    "Returns a list of the **top 10 most similar words** to `\"natural\"`, ranked by **cosine similarity** between their vectors.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```python\n",
    "[('the', 0.182), ('computers', 0.173), ('nlp', 0.168), ...]\n",
    "```\n",
    "\n",
    "These are the words that appeared in **similar contexts** to `\"natural\"` across the training data.\n",
    "<br/>\n",
    "\n",
    "üß† This similarity emerges from the way skip-gram **updates** the vector for `\"natural\"` and its surrounding words whenever they co-occur ‚Äî over time, semantically related words move closer together in vector space.\n",
    "\n",
    "#### üß≠ Summary\n",
    "\n",
    "| Component           | Role                                                     |\n",
    "| ------------------- | -------------------------------------------------------- |\n",
    "| Skip-gram Objective | Predicts context words based on a center word            |\n",
    "| sg=1                | Enables skip-gram architecture in `Word2Vec()`           |\n",
    "| Output Vector       | A 100D numeric representation of the word \"natural\"      |\n",
    "| Similar Words       | Top 10 words that share similar context to \"natural\"     |\n",
    "| Math Mechanism      | Uses dot products and softmax to update vector positions |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipGram.save('saveModelSkipGram.bin')\n",
    "# Word2Vec.load('path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üíæ Saving and Loading a Word2Vec Skip-gram Model\n",
    "\n",
    "#### ‚úÖ What This Code Does\n",
    "\n",
    "#### üìå `model_skipGram.save('saveModelSkipGram.bin')`\n",
    "\n",
    "* This **saves** the trained Word2Vec Skip-gram model to a file named `'saveModelSkipGram.bin'`.\n",
    "* You can later load this file to **reuse the model** without retraining it.\n",
    "* Useful for large models that take time to train.\n",
    "\n",
    "#### üìå `Word2Vec.load('path')`\n",
    "\n",
    "* This **loads** a previously saved Word2Vec model from disk.\n",
    "* You can then use the loaded model to get vectors or find similar words.\n",
    "\n",
    "#### üß† Example\n",
    "\n",
    "```python\n",
    "# Save the model\n",
    "model_skipGram.save('saveModelSkipGram.bin')\n",
    "\n",
    "# Later or in another notebook\n",
    "from gensim.models import Word2Vec\n",
    "loaded_model = Word2Vec.load('saveModelSkipGram.bin')\n",
    "\n",
    "# Use the loaded model\n",
    "loaded_model.wv['natural']\n",
    "```\n",
    "\n",
    "#### üóÇÔ∏è Summary\n",
    "\n",
    "| Action    | What It Does                      |\n",
    "| --------- | --------------------------------- |\n",
    "| `.save()` | Saves the trained model to a file |\n",
    "| `.load()` | Loads the model back into memory  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use word2Vec for SkipGrams and CBOW? Explore whether they will give different results for similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Clustering Words based on Cooccurence Pattern - **Brown Clustering**\n",
    "\n",
    "> Brown Clustering is a method to cluster words based on their co-occurrence patterns.\n",
    "\n",
    "> It starts with each word as a separate cluster and iteratively merges the most similar clusters based on their co-occurrence patterns.\n",
    "\n",
    "> Brown Clustering is used for tasks like named entity recognition, word sense disambiguation, and topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'brown' tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to c:\\Users\\mathe\\Downloads\\Embe\n",
      "[nltk_data]     ddingClusteringVectorizationWorkshop\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "\n",
    "# download brown corpus if not downloaded before\n",
    "# Warning: This will download the brown corpus to your home directory.\n",
    "# For example, on Linux, it will copy files to ~/.nltk_data.\n",
    "# In Windows, it will copy files to C:\\Users\\YourAccount\\AppData\\Roaming\n",
    "# nltk.download('brown')\n",
    "\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "print(\"Downloading 'brown' tokenizer...\")\n",
    "nltk.download('brown', download_dir=nltk_data_path, force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ü§ù Co-occurrence and Brown Clustering: An Introductory Example\n",
    "\n",
    "### üìå Code Recap\n",
    "\n",
    "This code performs a **simple version of Brown clustering** based on a **co-occurrence matrix** built from the [**Brown Corpus**](https://en.wikipedia.org/wiki/Brown_Corpus).\n",
    "\n",
    "---\n",
    "\n",
    "### üî§ What Is Co-occurrence?\n",
    "\n",
    "> **Co-occurrence** means that two words appear **near each other** in a sentence ‚Äî within a defined **window size**.\n",
    "\n",
    "For example, in the sentence:\n",
    "\n",
    "```\n",
    "\"The field of natural language processing is growing.\"\n",
    "```\n",
    "\n",
    "If the window size is 2, the word `\"natural\"` co-occurs with:\n",
    "\n",
    "* `\"of\"`, `\"language\"`, `\"field\"`, and `\"processing\"`\n",
    "\n",
    "---\n",
    "\n",
    "#### üßÆ What the Code Does Step by Step\n",
    "\n",
    "#### 1. üì• Download and Load the Brown Corpus\n",
    "\n",
    "```python\n",
    "nltk.download('brown')\n",
    "corpus = brown.sents()[:1]  # use just one sentence for this example\n",
    "```\n",
    "\n",
    "* Retrieves one sentence from the **Brown Corpus**.\n",
    "* Converts all words to **lowercase**.\n",
    "\n",
    "#### 2. üßæ Build a Vocabulary\n",
    "\n",
    "```python\n",
    "vocab = set(word for sent in corpus for word in sent)\n",
    "```\n",
    "\n",
    "* Creates a set of **unique words** in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve sentences from brown corpus\n",
    "corpus = brown.sents()[:1]\n",
    "# transform all sentences to lower case\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]\n",
    "\n",
    "# Create a set of unique words in the corpus --> Vocab\n",
    "vocab = set(word for sent in corpus for word in sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. üß± Create a Co-occurrence Matrix\n",
    "\n",
    "```python\n",
    "window_size = 2\n",
    "co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "```\n",
    "\n",
    "* Builds a **square matrix** of size *(vocab\\_size √ó vocab\\_size)*.\n",
    "* Each cell $(i, j)$ in the matrix counts how often **word i** appears within the **window** of **word j**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find co-occurence pattern, co-occurence matrix is needed to show the word count and which words does it co-occur with\n",
    "window_size = 2  # window_size (bigram)\n",
    "co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "for sentence in corpus:\n",
    "    for i, word in enumerate(sentence):\n",
    "        for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "            if i != j:\n",
    "                word_index = list(vocab).index(word)\n",
    "                context_word = sentence[j]\n",
    "                context_word_index = list(vocab).index(context_word)\n",
    "                co_occurrence_matrix[word_index, context_word_index] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. üß† Simulate Brown Clustering\n",
    "\n",
    "```python\n",
    "num_clusters = 2\n",
    "...\n",
    "```\n",
    "\n",
    "* Assigns each word to its own cluster initially.\n",
    "* Iteratively **merges the two most strongly co-occurring clusters**.\n",
    "* This continues until only the desired number of clusters remains.\n",
    "\n",
    "#### 5. üì¶ Final Output\n",
    "\n",
    "```python\n",
    "final_clusters[word] = cluster_id\n",
    "```\n",
    "\n",
    "* Produces a dictionary of `{word: cluster_id}` showing the grouping of words.\n",
    "\n",
    "#### üìä What Is a Co-occurrence Matrix?\n",
    "\n",
    "It‚Äôs a matrix where:\n",
    "\n",
    "* **Rows and columns = words in vocabulary**\n",
    "* Each cell $M_{i,j}$ contains the **count of times word $i$** appeared near **word $j$**\n",
    "\n",
    "$$\n",
    "M_{i,j} = \\text{Number of times } w_i \\text{ appears in the context window of } w_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing brown clustering, you need to set the number of clusters\n",
    "num_clusters = 2\n",
    "# assign each word as cluster\n",
    "cluster_assignments = np.arange(len(vocab))\n",
    "# Perform Brown clustering by recursively merging clusters\n",
    "for k in range(len(vocab) - num_clusters):\n",
    "    # Find the pair of clusters with the highest co-occurrence count\n",
    "    i, j = np.unravel_index(co_occurrence_matrix.argmax(), co_occurrence_matrix.shape)\n",
    "    # Merge the clusters by assigning the same cluster ID to both clusters\n",
    "    cluster_assignments[cluster_assignments == j] = i\n",
    "    # Update the co-occurrence matrix by merging the counts of the two clusters\n",
    "    co_occurrence_matrix[i, :] += co_occurrence_matrix[j, :]\n",
    "    co_occurrence_matrix[:, i] += co_occurrence_matrix[:, j]\n",
    "    co_occurrence_matrix[i, i] = 0  # Set diagonal element to 0\n",
    "    co_occurrence_matrix[j, :] = 0\n",
    "    co_occurrence_matrix[:, j] = 0\n",
    "\n",
    "# Final cluster assignments\n",
    "final_clusters = {}\n",
    "for word, cluster_id in zip(vocab, cluster_assignments):\n",
    "    final_clusters[word] = cluster_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß≠ Summary\n",
    "\n",
    "| Step                      | What It Does                                  |\n",
    "| ------------------------- | --------------------------------------------- |\n",
    "| Corpus                    | Loads a sentence from the Brown Corpus        |\n",
    "| Vocabulary                | Extracts all unique words                     |\n",
    "| Co-occurrence Matrix      | Counts how often words appear near each other |\n",
    "| Brown Clustering (simple) | Merges similar words into clusters            |\n",
    "| Final Output              | A dictionary of word-to-cluster mappings      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(final_clusters.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: DONT RUN FOR LARGE DATASET**<br>\n",
    "Visualizing brown clusters: <br>\n",
    "Install scipy if you didn't use it before. pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_14824\\3430164255.py:6: ClusterWarning: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  linkage_matrix = linkage(co_occurrence_matrix, method='ward')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABQQAAANXCAYAAABjVFL4AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAazRJREFUeJzt3QeUldXZP+x7YAREGLCixIJYQOlFsKFoBKImFvRvjYrdJEYxGnsEy2sSNWosb8SGscUexGis0YiVElGxi6IGNRqFARVEYL6197tmPkA6wwzMc11rnXWe89T9nDnHcX7ce++SioqKigAAAAAACqFebTcAAAAAAKg5AkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAgAIYPHhwlJSUxMoqtT3dA3MbMGBAtGrVqrabAQCsZASCAECdc9NNN+UAac7HOuusEzvttFP8/e9/j7pk+vTpcdlll0XPnj2jWbNm0ahRo9h8883j+OOPj7fffrvG2nH77bfH5ZdfHnXNhAkT5vocrbLKKrHWWmvFtttuG2eeeWZ8+OGHtd1EAIAlVrrkhwAArBzOO++82HjjjaOioiL+85//5KBwt912iwceeCB+/OMfx8ruv//9b/zoRz+KMWPG5Ps56KCDokmTJvHWW2/FHXfcEddee23MmDGjxgLBcePGxcCBA5fL+adNmxalpbX3v64HHnhg/uzMnj07Jk2aFKNGjcoB6B//+Me44YYb4oADDqi1tgEALCmBIABQZ+26667RvXv3qtdHHnlktGjRIv7yl78sNBCcOXNmDn4aNGgQK3p30Zdeeinuueee2Geffebadv7558dZZ50VK7P0M0iBZqp6TI/a1LVr1/jpT38617oPPvgg+vbtG4cddlhsscUW0alTp1gR1eTnec6fGQCw4tJlGAAojObNm8eqq646V6VZZZfQSy65JFd8bbLJJtGwYcN4/fXX8/Z//OMf0atXr1httdXy8XvuuWe88cYbVce/8sor+fjhw4dXrUsVe2ldCpHmDShT195Kaey3FEw+88wz0aNHjxyitG7dOm6++eZF3suLL74YDz74YA455w0Dk3QP6Z4WpPK+U9Xkosbrmzp1aq78S+1N503dr/v06RP/+te/8vbevXvntqSArLJr7Zzj2n377bcxaNCg2HTTTfPxG2ywQZx66ql5/bzXTV2db7vttmjXrl3e9+GHH55vmyrHRHz33XdzMJp+NqnL9OGHHx7ffPPN96oLTzjhhNzVt2nTprHHHnvExIkTl3lcwo022ii/fykAu+iii+baNnny5PyepXtN95Hu/fe//30OzOb9GaSfU6rmrPzsbbXVVrkCcV7Dhg2L9u3b589Jev7rX//6vX2W9fNc6amnnspherpWOseQIUPmOw7lwn5mqQ2pa/Waa66Zv3fdunXL4fW8Ks9x9913x5Zbbpn33WabbeLVV1/N29O10/uX2pI+a+keAYBlo0IQAKizysvLc7fa1GX4s88+iyuvvDK++uqr71V6JUOHDs3j8R1zzDE51FhjjTXi8ccfzyFeCulSGJKCpXSO7bbbLodhKfRKwUwKVp5++ukcNCUjRoyIevXqxcsvvxxTpkyJsrKyHAQ999xz+fxzSoHWvvvum4O9VGl244035oArhScpYFmQygDykEMOieXtuOOOy0FOCm1SYPPFF1/kEDMFSSn0TJWI6b3+97//ncczTFLX5STdd3pf0v7p3lMlXQp60n5pjMMUcs0pBVZ33XVXvlYK8BY1YcZ+++2Xu4X/9re/zT+T66+/PgeWKXyrlN7PdM70Xm299dbxz3/+M3bfffdqeW9ScJUCs8cee6xqXQokd9xxxxw6HnvssbHhhhvmn/0ZZ5wRn3zyyffGWkzdrVPomvZN4VgKF/v37x/vvfdeHrMwefTRR3Pwm97/dK/pZ5DCz/XXX3++7Vraz3OSqk5TV/T11lsvzj333Jg1a1bufr/22mvP91oL+pml7tTpZ3/wwQfn0DR1Y/9//+//xd/+9rfvvf/pO5M+07/4xS/y63SPKSxPwfH//u//xs9//vPcVTu9N0cccUS+JgCwDCoAAOqYoUOHVqT/zZn30bBhw4qbbrpprn3ff//9vK2srKzis88+m2tb586dK9ZZZ52KL774omrdyy+/XFGvXr2KQw89tGrd7rvvXtGjR4+q1/3798+P+vXrV/z973/P6/71r3/l69x///1V+2200UZ53dNPP121LrUhtfPkk09e6D3uvffe+dhJkyYt1nsyaNCgvP+8953eq3ml9Wn/Ss2aNav4xS9+sdDzp/cg3c+8brnllvx+jRgxYq7111xzTb7Os88+O9d1076vvfbaIttUeT9HHHHE996XNddcs+r1mDFj8n4DBw6ca78BAwZ875zzU/k+XXzxxQvcZ88998z7lJeX59fnn39+xWqrrVbx9ttvz7Xf6aefnj8TH3744VznTu398ssvq/ZLn5G0/oEHHpjrs7jeeutVTJ48uWrdo48+mveb832vjs/zT37yk4rGjRtXTJw4sWrdO++8U1FaWjrXZ2hRP7NvvvlmrtczZsyoaN++fcXOO+/8vXOkz3xqe6UhQ4bk9euuu27FlClTqtafccYZef2c+wIAS06XYQCgzrr66qtz5VZ63HrrrXmW4aOOOiruu+++7+2bqq/mrIBKlVxjx47N1WWpuqpSx44dc3fZhx56qGpd6oKZKqy+/vrr/DpVw6UJKDp37pwrn5L0nKq/tt9++7mumyq+0vGVUhvatGmTq8MWJlUeJqkL7PKWKiBTF+WPP/54iY9N3UBTVWDbtm1ztWblY+edd87bn3zyybn2T5V16T1ZkurFOaX3MlXPVb4/ld1XU4XZnH75y19GdamshkxVfpX3nNqx+uqrz3XPu+yyS662S9Wkc9p///3zvnPeQ1L5Gaj8LKYK0tQtulL6HC7ovVraz3NqX6ok3GuvvaJly5ZV+6Uuu6m6cH4W9DNLXX8rpeq+VEVa+V2Z1w9/+MO5qkEru9an+5jzM165flHfDwBg4XQZBgDqrDQu35yTiqSZYrt06ZK7NqbuiHNOspC6nc4pjYeXpHBuXingeuSRR3IAmMZiSyFHmrjh+eefz2PGpe7Jad1rr702VyCYQpM5w5gkdSedVwqHUoCyMKkbcmUIlQK75Sl100xhVLq31JU5hZ2HHnpo7nq6KO+8807uWryg7qbpvZrTvD+HRZn3/asM1tL7l96j9HNM3bfnPW8KuKpL6oaeVAZX6Z7T2JKLe88Lu4c5P4ubbbbZ986VPp/zC9iW9vOcgtTUlXh+78+C3rMF/cxS1+ALLrggB5Fzjhc57ziE83sPKoPP9Jmb3/pFfT8AgIUTCAIAhZGCoVQlmMY2S6HNnGP0zVnNtKQqJ19IlV8p2Ehj2G2++eY5FEzjn6UwJAWCe++99/eOrV+//nzP+X89KRcsVdwlaTy+OSsMF9f8QpnKCrH5jdOXrpEmsUhj2V188cV5jL5UabmgqrFKaQzBDh06xKWXXjrf7fMGPkv6c1ja9686jRs3Lv/MK0PadM+p6i6Nfzc/6bOxvO9hWT7P1XGt9HlP4wfusMMO+TuQxiNM4yGmsQ3TmInzWtB7sCL8fAGgLhIIAgCFkir55qzqWtgMsslbb731vW1vvvlmnjwhVQcmqdIwVSOmECQFgpUBXXpOYWCagfU///lPDkeqy09+8pM88ULqCr00gWBlFVqaDXd+lWTzSoFO6nabHqnCLU0m8j//8z9VgeCCAsY04UaaXCV1CV3QPstT+jmmgO7999+fq8IuTeZSHVJV6Pjx4+eaqCbdc/p8pS7C1aHys5hC7HnN7/O5LJ/nFGynx/zenyV5z+699958nlR5mCY1qZQCQQCg9hlDEAAojO+++y5XuKUAL3WTXJgUgKUxAP/85z/PFZqlarB0jtRtdk4plEvj7KUx8SoDuhSypOtUzni7NMHdwma3TTPBpll1552pN0mzup5yyikLPD5Vs6X2zTueXarmmrdiMI39NqdUDZfGl5uzG2gKk+bdr7K6MM22e911131vW+qaWjnu4vLSr1+/+d5Xml13WaXwNI3Jlz5Pv/71r+e65xQUpjBsXumzVBlKL645P4tzvsdpbMzXX399ic+xsM9zqshLQWb6TM05ZmQKA//+978vdpvTeVIAPGfF6YQJE+b7WQUAap4KQQCgzkoBRqp+SlJVW+qqmKqsTj/99KrunQuTusamCrgUvh155JE5wEpBUhrHbPDgwXPtm8K+VDH30UcfzRX8parAIUOG5AkT1l9//Wq9v5tvvjn69u0b/fv3zxWDqQovBXPpHu+44448kcQll1yywOPTBCu/+93v8nPq9pzCwbfffnuufdIYhand++67b3Tq1ClPoJEmnRg1alT84Q9/qNovjS145513xq9+9avYaqut8n6pTYccckjcddddefKPFJZut912OSRKP5e0PoVmc47zWN1Su9LEFJdffnmebGTrrbeOf/7zn1X3ubhVi2mcvlSNmaoNU6CW7j9VwaXjb7nlljw5R6UUDg4fPjyPU5kCw9SGFHym7t333HNPDsZSGLskUjXo7rvvnielOeKII+LLL7/Mn8XU7X1R1a5L+nlOyykkTD+rn/3sZ/nnddVVV0X79u3zeICLI7U1dRNPofVBBx2Uv39pkp80DmEaXxEAqF0CQQCgzjrnnHOqllP3xTTu3p/+9Kc49thjF+v4VCmVZqkdNGhQPlcaAy3NqJoq/uadSGHbbbfNVVGNGzfOwVmlFA6mQLA6qwMrpUkrnnvuuVz9lsK4s846K1cGpu6hafy2E088caHHp3v6/PPPc0iVwrkUFqUQNVUAVkr3k7oJp4AojRmYArEU6qRrprCoUtonhUWpS+hll12W25ACwTRuY6oKS+tSgJnGIUznTBOSpPbNO57e8pCuu+6668Zf/vKXfP30c03vV5pgI30uFkc6Nj1KS0tzmJy6Hw8cODAHnfNOiJHuL4WOF154YZ5xOF0/HZPu9dxzz51rpuDFlYK1dK6zzz47zjjjjNwtOb3X999/fzz11FPV+nlOAWb6HKQK09/85jd5nMfzzjsvTw5TGbAvSppF+oYbbsiBc3qf0vnTdVIYKhAEgNpXUmFEXgAACiaFl2nG6VT1d/DBB9d2c1YKe+21V545e35jGQIAKxdjCAIAUKelrrHzSl2IU/VidU70UpffsxQCPvTQQ9G7d+9aaxMAUH10GQYAoE676KKLYsyYMbHTTjvlLr+pO2x6HHPMMbk7LN+XunSn8Q/Tc5o8JXW1T5OnnHrqqbXdNACgGugyDABAnZZm401j96UZedMEHGnMvzTZSRpzMQWEfN/hhx+eJ4H59NNPo2HDhnkikjQmYteuXWu7aQBANRAIAgAAAECBGEMQAAAAAApEIAgAAAAABWLQlJXY7Nmz4+OPP46mTZtGSUlJbTcHAAAAgFqSRgWcOnVqtGzZMurVW3gNoEBwJZbCQDPjAQAAAFDpo48+ivXXXz8WRiC4EkuVgZU/6LKystpuDgAAAAC1ZMqUKblwrDIvWhiB4EqssptwCgMFggAAAACULMawciYVAQAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQguwPDhw+Okk06q7WYAAAAAQLUqqaioqKjeU678Zs6cGaWlpcv1GrNnz87P9eotfSY7ZcqUaNasWZSXl0dZWVk1tg4AAACAlcmS5ESFqhAsKSmJs88+O7p06RKbb7553HbbbXNtGzRoUGy11VZxxhlnxE033RR77bVX3vbUU09F+/bt42c/+1l07NgxOnToEK+88koMGDAgL/fs2TMmTpyY93311Vdj++23j65du8aWW24ZF1xwQdU1Bg8eHPvss0/069cvn++WW26Jvn37Vm2fNWtWbLTRRvH666/X6PsCAAAAQHEUKhCsDP5eeumlePjhh+OXv/xlTJgwoWpb/fr1Y9SoUXHxxRd/77g333wzjjrqqBwEpqBw5513jtNPPz0HgN27d4/LL78879eqVat44okn4l//+leMGTMm7r333njhhReqzvP888/HzTffnEO/n/70p/H222/HW2+9VdVNedNNN81B4hKZ8fXSvyEAAAAAFErhAsEU6iWtW7eOHXbYIZ5++umqbUccccQCj0tBXbdu3fJyCgDT67Zt2+bXPXr0iHfeeScvT5s2LV8jVQ5uvfXW8cEHH8TYsWOrzrPbbrtFixYtqgLIn//853H11Vfn1+n5+OOPXy73DQAAAADJ8h0obyWpGKzUpEmTBe7XqFGjquUU5M37Oo07mJx55pmx1lpr5SrENA5h//79Y/r06Qu8xtFHH50rAg899NB49913Y4899qi2ewMAAACAKHqF4NChQ/Nz6io8YsSI6NWrV7Wef9KkSbH++uvnMDB1BX7ssccWuv/qq68ee+65Z+y9995x7LHH5nARAAAAAJaXwlUIpok70qQiX3/9dVxxxRV5zL/qlCYtOeSQQ+LPf/5zbLLJJnmswUVJVYJpEpP0DAAAAADLU0lFRUVFFKh7cKrga968eaxILrnkknjjjTfihhtuWLrppD//OMrWWm+5tQ8AAACAFVtVTlReHmVlZQvdt3AVgiuadu3a5aAyzXoMAAAAAMtboQLBFbEY8rXXXqvtJgAAAABQIIWbVAQAAAAAikwgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQLAumDGttlsAAAAAwEpCIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBGvQ5ZdfHp9++mltNwMAAACAAhMI1iCBIAAAAAC1TSAYEc8//3xsv/320alTp+jYsWPcf//9MXr06Nh2223z6x49esSzzz6b950wYUI0b9686tivvvoqSkpKql6n5QsvvDAfs/HGG8fQoUPz+vPOOy8+/vjj2H///aNz587xwgsvxLrrrhsfffRR1bFnnnlmnHbaaTV67wAAAAAUS0lFRUVFFNiXX34ZW2yxRdxzzz3Rq1evmD17dvz3v/+N7t27x3XXXRf9+vWLZ555Jvbdd994991387YU6E2ePLkqEGzatGlUvo0pELzkkkvi5JNPjjfffDO22mqrmDRpUpSWlkarVq1i2LBh+fjkrLPOyselAPHbb7/N21NQuNFGGy1W26dMmRLNmjWL8s8/jrK11luO7xIAAAAAK7KqnKi8PMrKyha6b+ErBFN1YJs2bXIYmNSrVy/+85//5OcUBiaperBFixYxduzYxTrnwQcfnJ/btm2bg8AFdRP++c9/Hn/+859zGHj33XfnqsLFDQMBAAAAYGkUPhBcXJXdglPAN2vWrKr106dP/96+jRo1qlquX79+zJw5c77n/MEPfhA77LBD3HnnnXH11VfH8ccfv1zaDgAAAACVCh8IpnEC33nnnRgxYkR+nboMp2rA9PzYY4/ldc8991yu8ktdfdO4f6mb7+uvv5633XzzzYt9rVSumco253TiiSfmrsOpC/Iuu+xSrfcGAAAAAPMqfCC4+uqrx1//+tc4/fTT8wQiXbt2jRdffDHuu+++GDRoUF43cODAPMZgkyZNcoXglVdeGT/+8Y/z+IDffffdYl/rhBNOiKOPPjoHi5Xdj7feeuvcvzt1H55zchIAAAAAWB4KP6lIbZs4cWKewOTtt9/Ok5MsCZOKAAAAAJCYVGQlcc4550TPnj3jd7/73RKHgQAAAACwNFQIrsRUCAIAAACQqBAsmhnTarsFAAAAAKwkBIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgWI0OPvjg6N69e3Ts2DF23333+PTTT2PChAnRvHnzGDRoUHTr1i023XTTeOihh/L+l1xySRxzzDFVx0+ePDnWWmut+PLLL2vxLgAAAACoywSC1ejyyy+P0aNHxyuvvBK9evWKwYMH5/Xl5eU5JBwzZkxcddVVcdJJJ+X1Rx11VAwbNiwHgcnQoUNjzz33jDXWWKNW7wMAAACAuqu0thtQl9x+++1xyy23xPTp0/MjVfsljRo1iv79++flbbbZJsaPH5+XU+XgvvvuGzfeeGMOCf/0pz/FnXfeueQXbrBq9d4IAAAAAHWWQLCaPPPMM3HFFVfE888/H+uss04MHz48zjnnnLytYcOGUVJSkpfr168fs2bNqjruhBNOiD322CO22GKLWHvttaNLly61dg8AAAAA1H26DFeTSZMmRdOmTWPNNdeMGTNmxJAhQxbruLZt20br1q3zWILHH3/8cm8nAAAAAMUmEKwmP/rRj6JNmzb5kcYP7Ny582Ife/TRR8fMmTNz92EAAAAAWJ5KKioqKpbrFVikVBnYokWL+M1vfrNEx02ZMiWaNWsW5RPHR1nL1sutfQAAAACs2KpyovLyKCsrW+i+xhCsRR9//HHsvPPOeVbhRx55pLabAwAAAEABCARrUcuWLePNN9+s7WYAAAAAUCDGEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIALBFcigQYOibdu20bNnz9puCgAAAAB1VGltN4D/30UXXRTvvfderLfeerXdFAAAAADqKBWCteCRRx6Jrl27RseOHWPHHXeM119/PbbddtuYPn169O3bN0444YTabiIAAAAAdZQKwRr22WefxUEHHRRPPfVUdOjQIW677bbYd99947XXXot69erFiBEjonnz5rXdTAAAAADqKBWCNezFF1/MQWB6JAcffHB8/PHHMXHixKU/aYNVq6+BAAAAANRpAsG6YMa02m4BAAAAACsJgWAN23rrrePVV1+NcePG5dd33HFH/OAHP8gPAAAAAFjejCFYw9Zee+08buChhx4aM2fOjNVXXz3uvvvuKCkpqe2mAQAAAFAAJRUVFRW13QiWzpQpU6JZs2ZRPnF8lLVsXdvNAQAAAKC2c6Ly8igrK1vovroMAwAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKpNYDwc6dO8fUqVNrtQ0lJSUxefLkJT5u+PDhcdJJJ+XlCRMmxDXXXDPX9t122y3eeuutamsnAAAAACyrkoqKiopYAc2cOTNKS0urXs+ePTs/16tXb7kEgpMmTYrmzZsvdfueeuqpGDhwYIwdOzZqypQpU6JZs2ZRPnF8lLVsXWPXBQAAAGDFUpUTlZdHWVnZil0hOGd1XqtWreK0006LHj16xGGHHRaDBw+OffbZJ/r16xft27ePTz75JB555JHYfvvto1u3bnm/J598supcgwYNik033TS22mqrOPvss/P5Kqv35gz7vvrqq3zd+TnllFPy8alycYcddpirwi8dk66Rtp9xxhlx0003xV577ZW3HXfccXnfdNwee+xRdT+VAeGnn34a++23X25zhw4dcvsqg87jjz8+tthii+jUqVO+r+nTpy+HdxoAAAAAIv7/ErcVxBdffBEvvvhiDt9SIPj888/HSy+9FC1atIj33nsvr0uhYEo633333ejVq1cO/B5//PG49957875NmjSJI444YqmunwLJSy65JC/fcccdceKJJ8bDDz9ctb1+/foxatSovJwCwUqpu/DCKgRTwHnmmWfGjjvumKsLf/zjH8fdd9+dA8wnnngiXnvttVz9mFLcBg0aLFmjG6y6VPcKAAAAQPGscIHggAED5qreS+PwpTAwScFcCgFT5V6lFKJ9+OGHOVT7f//v/0XTpk3z+iOPPHKu6sHF9dhjj8WVV16ZxzVM1XtffvnlXNuXJmj8+uuvc/v+85//zFWlmCoK+/btmwPCdN6ddtopdt999yXvFj1j2hK3CQAAAIBiWuECwVTdt6DXabjDPn36xO23377I88wZKqax/mbNmlX1ekFdclOwmLrvpgrATTbZJF555ZW5wsf5tW9xVA7T+MILL0SjRo2+t33cuHHxz3/+MweYqSvy008/nSsHAQAAAKC61foYgksijSWYuganoK7SyJEj8/POO++cuwynyrsUwN14441V+6y77rp53euvv55f33zzzfM9f+quu8oqq8R6662X97/qqqsWu22pC3M6fn5SiJiq/373u99Vrfv444/j3//+d3z++ee5gjBVCl544YV53MHKdgIAAABAoQPBVDWXqgOPPfbYPAFHmojj8ssvz9vSmHx77rlnntQjTfqRJhGpnEgkVQimbsBpn7Ttu+++m+/502QfBxxwQLRr1y7vt+GGGy522zp27JiPS5OfVE4qMqfbbrstd3dO29N1+vfvn8dL/Oijj3LVYzo+bUuPXXfddanfIwAAAABYmJKKyv6sdUAa9y+NIZhu6eSTT45p06bFn/70p6jz00lPHB9lLVvXdnMAAAAAqO2cqLw892RdqcYQXBaHHnponnE4jRGYqvXSzL8AAAAAQB0NBP/617/WdhMAAAAAYIW2Uo0hCAAAAAAsG4EgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhCsJp07d46pU6fWdjMAAAAAYKFKF76ZxTV27Ngl2n/mzJlRWurtBwAAAKBmqRCsJiUlJTF58uRo1arVXOFg9+7d46mnnsrLvXv3jhNOOCG22Wab6Nu3bxx//PFx4YUXVu371ltvxQYbbJDDQgAAAABYHpSo1bC33347nn766VhllVVyANivX7847bTTon79+vG///u/ccwxxyx55WCDVZdXcwEAAACoY1QI1rCf/vSnOQxM2rRpE1tuuWXcf//98fXXX8df/vKXHAgusRnTqr+hAAAAANRJKgSrWarumzVrVtXr6dOnz7W9SZMmc70+8cQT4/e//318/vnn0adPn2jRokWNtRUAAACA4lEhWM023XTTePHFF/PyyJEjc7fghUljCX766adxwQUX5DEFAQAAAGB5EghW88QiKdi7+uqro1OnTnHjjTdGu3btFnnMkUceGeuss06ebAQAAAAAliddhqvBZ599Fg0aNIimTZvmWYVfe+21+e5XOdvwvJ588skYOHDgcm4lAAAAAKgQXGajRo2KrbfeOgYPHhz16i3Z2zl69OjcxTgdd9BBBy23NgIAAABApZKKioqKqlesVKZMmRLNmjWL8onjo6xl69puDgAAAAC1nROVl0dZWdlC91UhCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIILcf/998cWW2wRnTt3jldffXWubcOHD4+TTjppvseNGzcuWrVqVUOtBAAAAIDFV7oE+xbONddcE+ecc04ceOCBc62fOXNm7LHHHvkBAAAAACsTFYILcMIJJ8SIESPizDPPjG233TZKSkpi0KBBsdVWW8UZZ5wRN910U+y1115V+w8ePDg222yz6NatW9xxxx1zhYf9+vWL7t27R7t27eKggw6Kr7/+Om/78Y9/HLfffnvVvo8++mj07Nmzhu8UAAAAgCIRCC7AFVdckUO8yy67LJ577rm8rn79+jFq1Ki4+OKL59r3wQcfjLvvvjvGjBkTo0ePjgkTJlRtS8ek0C+tT12JmzVrFldeeWXeduKJJ8ZVV11Vte/VV18dxx9/fI3dIwAAAADFIxBcAkccccR81z/xxBOx3377RVlZWa4kPPbYY6u2VVRU5FCxS5cu0bFjxxwejh07Nm/r06dPlJeXx0svvRQffPBBjBw5Mp8HAAAAAJYXYwgugSZNmizWfikUrJSqA//xj3/EP//5zxwYpsrD9HrOrsmpYrBFixY5cGzYsOFyaTsAAAAAJCoEq8Euu+ySuwxPnTo1VwRee+21VdsmTZoUa621Vg4D0/Y09uCcDjnkkHjkkUdi6NChcdxxx9VC6wEAAAAoEhWC1WC33XbL3X27du2ag79dd921atuhhx4a999/f7Rp0ybWXnvt6NWrV+4eXKlx48bRv3//+Pjjj2ODDTaopTsAAAAAoChKKlJJG7Vm1qxZeWbi1G04hYVLYsqUKXmSkvKJ46OsZevl1kYAAAAAVmxVOVF5eS5YWxhdhmvR8OHDY5NNNoltttlmicNAAAAAAFgaKgRXYioEAQAAAEhUCAIAAAAA8yUQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAsEaUFJSEhdeeGH06NEjNt544xg6dGjVttGjR8e2224bHTt2zNufffbZWm0rAAAAAHVbaW03oCgaNmwYI0eOjDfffDO22mqrOOSQQ2L27NnRv3//uO6666Jfv37xzDPPxD777BPvvvtuNGnSpLabDAAAAEAdpEKwhhx88MH5uW3btlFaWhqffvppvPXWW1GvXr0cBibbb799tGjRIsaOHVvLrQUAAACgrhII1pBGjRpVLdevXz9mzpy5wO7FAAAAALC8CARrUZs2bXK34cceeyy/fu6553LlYOfOnWu7aQAAAADUUcYQrEUNGjSI++67L0444YQ4+eSTcxXhPffcY/xAAAAAAJYbgWANqKiomOv1f//736rl7t2758pAAAAAAKgJugwDAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAAqkVgPBzp07x9SpU2v0mk899VQ8/PDDVa8//vjj6NWr1zKfd/DgwTF9+vSq1+ecc07cdttty3xeAAAAAKhOJRUVFRVRICm4mzx5clx++eXVet6SkpKYNGlSNG/ePGrKlClTolmzZlE+cXyUtWxdY9cFAAAAYMVSlROVl0dZWdmKWyGYQrQUziWtWrXKVXXbbLNNbLzxxnHBBRfk9c8++2x06NBhruN69+4d999/f15+5JFHYvvtt49u3bpFjx494sknn8zr33nnndhuu+2iU6dO+fizzz47xo4dG9dcc02u3EvVieedd15MmDBhrhAvnXeLLbbIx5122mmx1lpr5X2SU045Jbbaaqt87A477BBvvfVWXn/cccfl51RpmLZ99tlnMWDAgKrQ8auvvoojjjgi2rdvnx/nnnvuXPeSzpuO3WSTTarOBQAAAADLQ2msQFI4+Pzzz8d///vfHI4dfvjhOdT79ttvY/To0dG9e/d47733chC3++675+VU8ZdCwZR8vvvuuzlYSwHeVVddFT/+8Y/jjDPOyOf+8ssvY4011siB25wVgpVhX5KCvBTcpRCybdu2MXTo0Pjiiy+qtqeA8JJLLsnLd9xxR5x44om5+3EKGYcMGRIjRoyYb4Xg+eefn+/hlVdeiWnTpuUAM51///33z9vHjx+fg8zvvvsuttxyy/wepGAUAAAAAOr0pCIHHXRQfk5Vea1bt473338/v07BYArnkj//+c9x8MEHR2lpaQ7jUgiYqvVSZd6+++4b9erViw8//DCvu+666+Kss86KRx99dLG68r7wwgvRsWPHHNYlhx12WDRo0KBq+2OPPZaDulTll6oLU8Xh4nj88cfj6KOPzm1bbbXV4tBDD83nqpSCwXQ/q666ar6PFBACAAAAQJ2vEGzUqFHVcv369WPmzJlVwVzqwpuq826++eb429/+lten4Q/79OkTt99++/fOtdlmm8W2226bg7dULZgqAh966KGlblsKGY8//vgYNWpUrl5M1X4pdFzartKLc98AAAAAUKcrBBekZcuWeey+k046KdZZZ51o165dXt+vX79cfZfCuUojR46sGkOwRYsWuRrvoosuytV/SepanAZXnJ+tt946n6tybMBbb701ZsyYkZfTMausskqst956OYhMIeOcmjZtusDz7rLLLnHDDTfk477++uu45ZZbom/fvtXy3gAAAABAnQsEK7sNp3H60nOlTTfdNFcHHnvssbmCME0GUjk24D333JMnE+nSpUvukpvG+Uv23nvv3NW3clKROaWw8frrr4+99torb3/11VejSZMmubtxOtcBBxyQw8gUTm644YZzHXvyySfnasXKSUXm9Jvf/CaHiekcPXv2jD322CP222+/5fhuAQAAAMD8lVSksjWqTJ06NVf7JcOGDcuTkrzxxhuxQk8nPXF8lLVsXdvNAQAAAKC2c6Ly8txDdqUZQ3BFcOWVV8add94Zs2bNym/ebbfdVttNAgAAAIBqo0JwJaZCEAAAAIAlrRBcacYQBAAAAACWnUAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBFcArVq1ys+9e/eOCRMm1HZzAAAAAKjDBIIAAAAAUCACwRXA2muvnZ/XWGONqF+/fm03BwAAAIA6rLS2G0DEqFGj8vN9991X200BAAAAoI5TIQgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACKVwgOHny5Pjd734317revXvHsGHDlum86fgXXnhhsfZ96qmnYsCAAct0PQAAAABYGgLBarIkgSAAAAAA1JY6HQgefPDB0b179+jYsWPsvvvu8emnn8Zxxx0XU6dOjc6dO+dt87r99tujZ8+e0aVLl+jUqVM88MADc1USnnLKKdGrV6/YZJNN8rmShx56KIYPHx4XX3xxPu/111+fr7XTTjtFt27dol27dnH88cfH7Nmz8/4NGjSIZs2a5eV33nkntttuu3ytDh06xNlnn11j7w8AAAAAxVNSUVFREXXU559/HmuvvXZeTlWBEyZMiNNPPz2HdqlScM6gb+DAgbHXXnvFF198EWussUaUlJTk/bfeeuv44IMPomHDhnm/1VdfPe6+++747rvvYsstt8wB4jbbbJO7AKfzpvMk06dPj5kzZ0aTJk1i1qxZseeee8ZPf/rTOOCAA+Zq44knnhjrrrtunHHGGfn1l19+ma+/OKZMmZKDxfKJ46OsZetqfOcAAAAAWJlU5UTl5VFWVrbQfUujDkth3S233JLDufRYa621FnnM+++/nysL//3vf0dpaWkO6NK6tm3b5u37779/Xp8eKQAcP358DgTnlaoBTzvttHjmmWciZa6fffZZtG/f/nuB4A477BC//vWv46uvvoodd9wxdtlll2p8BwAAAACgIF2GUxB3xRVX5O6848aNi0svvTSHgouSArujjjoqHzN27Nhc4TfncY0aNaparl+/fq4CnJ90vRQCvvjii/HKK6/EQQcdNN/r77PPPvHss89GmzZt4qqrroof//jHS33PAAAAAFDYQHDSpEnRtGnTWHPNNWPGjBkxZMiQvD6VTE6bNi2vW9BxG2+8cV6+9dZb8+vFkc6bSjLnPE/qCpwCxDSeYOpmPD9pDMEWLVrEoYceGhdddJGJSQAAAABYrupsl+Ef/ehHOdBLlXcpFExdcSdOnJjH50vhW5poJFX/jR49eq7j/vjHP8a+++4bzZs3j5133jk23HDDxbreIYcckscRTLMN/+IXv8hjA6bzpAlFWrZsucCuwPfcc09uZ5poJHUzvuaaa6rl/gEAAACgcJOK1HUmFQEAAABgSScVqbNdhgEAAACA7xMIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAcBkNHjw4pk+fnpcHDBgQl19++RKfY9iwYfHCCy8sh9YBAAAAwNwEgsvo3HPPrQoEl5ZAEAAAAICaIhBcBscdd1x+7tWrV3Tu3Dk+++yzeOONN+KHP/xhbL755tG/f/+YMWNG3ueJJ56IbbbZJrp06RLt2rWLG264Ia9/6KGHYvjw4XHxxRfnc1x//fW1ek8AAAAA1G0lFRUVFbXdiJVZSUlJTJo0KZo3b567DKdA8Mknn4yGDRvGDjvsEMcff3wceOCBeZ+ysrKoX79+fPnllzkYfPbZZ2P99dfPx6UwcODAgUt07SlTpkSzZs2ifOL4KGvZerndIwAAAAArtqqcqLw8Z1ALU1pjrSqIvffeOxo3bpyXe/ToEePHj8/LX3zxRRx55JHx9ttvR2lpaX49bty4HAgCAAAAQE3RZbiaNWrUqGo5VQPOnDmzqnvx9ttvH6+++mqMHTs2dyle1rEHAQAAAGBJCQSXUdOmTXMp5qKkLsMbbbRR7mL89NNPx8svv1y1LZVxLs45AAAAAGBZCQSX0cknnxx9+vSpmlRkQX73u9/F6aefnve78cYbo2fPnlXbDjnkkLjrrrvyuIImFQEAAABgeTKpyErMpCIAAAAALOmkIioEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFUq86pjQeNmxYvPHGG9XTIgAAAABgxQkE99tvv7jqqqvy8rRp06J79+55XceOHePee+9dHm0EAAAAAGorEHz66aejV69eefmvf/1rVFRUxOTJk+OKK66ICy64oLraBQAAAACsCIFgeXl5rLHGGnn54Ycfjn322ScaN24cu+++e7zzzjvLo40AAAAAQG0FghtssEE8//zz8fXXX+dAsG/fvnn9pEmTolGjRtXVLgAAAABgOShd0gMGDhwYBx98cDRp0iQ22mij6N27d1VX4g4dOiyPNgIAAAAAtRUI/vznP48ePXrERx99FH369Il69f6vyLB169bGEAQAAACAFVxJRZoVhJXSlClTolmzZlE+cXyUtWxd280BAAAAoLZzovLyKCsrW/YKwV/96leLffFLL710sfcFAAAAAGrWYgWCL7300lyv//Wvf8XMmTOjTZs2+fXbb78d9evXj27dui2fVgIAAAAANRcIPvnkk3NVADZt2jT+/Oc/x+qrr141w/Dhhx8evXr1qp5WAQAAAAArxhiCP/jBD+LRRx+Ndu3azbV+3Lhx0bdv3/j444+ru40sgDEEAQAAAFjSMQT/b4rgJTz5559//r31ad3UqVOX9HQAAAAAQA1a4kBw7733zt2D77vvvvj3v/+dH/fee28ceeSR0b9//+XTSgAAAACg5sYQnNM111wTp5xyShx00EHx3Xff/d9JSktzIHjxxRdXT6sAAAAAgNqvEJw1a1aMHj06/ud//ie++OKLPPtwenz55Zfxv//7v7HaaqvFyiiFnAsKM//2t79F7969a7xNAAAAAFDrFYL169fPE4e88cYbsfHGG0fHjh2jLjjuuONquwkAAAAAsGKOIdi+fft47733YkU2atSo2HnnnaN79+7RpUuXuPvuu+Poo4+OSy65pGqf999/P9Zdd93c7Xnw4MExcODAvD69/vnPfx6bbbZZ9OjRI5588sm5zn3LLbdEz549o2vXrrHDDjvEyy+/nNffdNNNscsuu8SBBx4YHTp0yNee830aOnRodO7cOTp16pS3TZgwIa9/5JFHYvvtt49u3brN93oAAAAAUKtjCF5wwQV5DMHzzz8/h1jzdhNe1LTGy9vkyZPjmGOOiYceeijWW2+9+O9//5vDu7/85S9x7LHH5rZXBngHH3xwrLLKKnMdf+2118Zbb70Vr732Wn7dr1+/qm3PPvtsPs/TTz8dDRs2jBEjRuSxFCv3TUHk2LFjc/Xk6aefHr///e9jyJAh8dRTT8V5550Xzz33XG7TN998k/dPgWEKI1MomN63d999N3r16pXDwnR+AAAAAKj1QHC33XbLz3vssUeUlJRUra+oqMiv0ziDtSmFbilo23XXXeda/+2338bMmTNzaJcq9G6++eZ44IEHvnf8E088EYceemg0aNAgvz7iiCPihhtuyMv3339/rghMFYKV0viJ06ZNy8vbbLNNDgMrl6+88sq8/OCDD8YhhxySw8CkcePG+fnhhx/OIWCqNKxUr169+PDDD3OFIgAAAADUeiC4ondpTcFku3btcjA4r8MPPzx33f3qq69irbXWyt2fF2Xe0POwww6LCy+8cL77NmrUaK7xFlMAuai29unTJ26//fZFtgMAAAAAamUMwR133HGhj9q27bbb5vEBH3/88ap1qRvvjBkzcpVeGk8wzSqcKv/mJ40DeOutt+axBNMxKUCslKoi07ZUwZfMnj07z7q8KD/5yU/ycZ988kl+nboMp0fqjpza+corr1TtO3LkyGW6fwAAAACo1grBynH6UjfaNNtwkiryUsDWrFmzqG2rr7567qKbxgo8+eSTc7C34YYbxrBhw6Jly5Z54o7hw4fnsf3mJ00+Mm7cuNhyyy3zudKYfmPGjMnb0vJFF10Ue++9d67+S4Hh7rvvnrsgL0zqEjxo0KAcAKaKw9Qd+Z577olNN900VwemsQ1TQJjOlyZBUTEIAAAAwPJSUpH6rS6BVBGXgq1VV101h2tJGpcvjaP36KOP5gk8qBlTpkzJIWz5xPFR1rJ1bTcHAAAAgNrOicrLFznp7xIHgqlKLlW2XXfddVFa+n8Fhqla7qijjsqTeaQZeKkZAkEAAAAAljQQXOIuw6lCcM4wMJ+ktDROPfXURXadBQAAAABWsklFUsJYOanGnD766KNo2rRpdbULAAAAAFgRAsH9998/jjzyyLjzzjtzCJged9xxR+4yfOCBBy6PNgIAAAAA1WSxuwy///77sfHGG8cll1ySZ8o99NBD89iBaQjCNGvuz372s/jd735XXe0CAAAAAGozENxkk01io402ip122ik/3n333Zg8eXLVtsaNGy+P9gEAAAAAtREI/uMf/4innnoqP/7yl7/EjBkzonXr1rHzzjvnR+/evaNFixbV2TYAAAAAoJqVVKQ+v0to+vTp8dxzz1UFhCNHjozvvvsu2rZtG6+99lp1t5FFTSc9cXyUtWxd280BAAAAoLZzovLyPClwtQeClVKV4LPPPht///vfY8iQIfHVV1/FrFmzlvZ0LCGBIAAAAABLGggudpfhygDwhRdeiCeffDJXBr744ouxwQYbxA477BBXXXVV7LjjjktyOgAAAACghi12IJjGCUwBYJppOAV/xx57bNx+++2x3nrrLd8WAgAAAAA1HwiOGDEih3+VE4ikUHDNNdesvpYAAAAAAMtdvcXdcfLkyXHttddG48aN4/e//320bNkyOnToEMcff3zcc8898fnnny/flgIAAAAAy2ypJxWZOnVqPPPMM1XjCb788sux2Wabxbhx45a9VSwWk4oAAAAAsKSTiix2heC8VltttVhjjTXyY/XVV4/S0tJ44403lvZ0AAAAAMCKNIbg7NmzY/To0bkaMFUFPvvss/H111/HD37wg9hpp53i6quvzs8AAAAAQB0IBJs3b54DwHXXXTcHf5dddlmeXGSTTTZZvi0EAAAAAGo+ELz44otzELj55ptX39UBAAAAgBUzEDz22GOXb0sAAAAAgOVuqScVAQAAAABWPgJBAAAAACgQgWAtOOyww+LGG2+sev3RRx9Ft27d4pNPPqnVdgEAAABQ9wkEa8H1118fDzzwQLz88sv59QYbbBA33XRTHqdx5syZtd08AAAAAOqwkoqKiorabgRLZ8qUKdGsWbMonzg+ylq2ru3mAAAAAFDbOVF5eZSVlS10XxWCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQXAaDBw+O6dOnL/XxEyZMiObNm1drmwAAAABgYQSCy+Dcc89dpkAQAAAAAGqaQHApHXfccfm5V69e0blz5/jss8+if//+0aFDh2jfvn0MGTKkat/Ro0fHtttuGx07dowePXrEs88++73zzZgxI37605/GscceG7NmzarRewEAAACgOASCS+maa67JzyNGjIixY8fGL3/5y2jTpk28+uqr8Y9//CMuuOCCeOGFF3LQl4LCQYMGxSuvvBKXXnpp7LPPPvHVV19VnWvy5Mnxox/9KLbccsscJNavX78W7wwAAACAuqy0thtQVzz++OMxZsyYvLzOOuvkEDCtW2211aJevXrRr1+/vG377bePFi1a5BBx/fXXz4HhdtttF6eddloceuihtXwXAAAAANR1KgSXk5KSksXatsoqq+SQ8IEHHojvvvuuhloHAAAAQFEJBJdB06ZNo7y8PC/vsssucd111+Xlzz//PO67777o06dP7kY8e/bseOyxx/K25557Lj799NM87mBlOJi6CW+wwQax1157xbRp02rxjgAAAACo6wSCy+Dkk0/OoV8K96644op444038qQiO+20U5x11lnRs2fPaNCgQQ4H0xiCaVKRgQMHxj333BNNmjSZ61xpbMGuXbvGbrvtNtf4ggAAAABQnUoqKioqqvWM1JgpU6ZEs2bNonzi+Chr2bq2mwMAAABAbedE5eVRVla20H1VCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQguIIYNmxYvPDCC7XdDAAAAADqOIHgAsycObNGrycQBAAAAKAmCATnUFJSEoMGDYqtttoqzjjjjJg6dWocffTR0aNHj+jYsWMcc8wxMWPGjLzvxIkTY999940OHTrkbb/5zW/y+oUd07t37zjllFOiV69esckmm8Rxxx2X1z/00EMxfPjwuPjii6Nz585x/fXX1+K7AAAAAEBdJhCcR/369WPUqFE5nDv55JNzeDdy5Mh4+eWXY/bs2fHHP/4x7/fTn/40unXrFq+++mq88sorccIJJ+T1CzsmGT9+fDz55JMxbty4eOSRR+L555+P3XbbLfbYY4/49a9/HWPHjo2jjjqq1u4fAAAAgLqttLYbsKI54ogj5urGmwK7Sy+9NL+eNm1aDgy/+uqreOaZZ3KgV2nttdde6DGV9t9//ygtLc2PVA2YAsJtttmmBu8QAAAAgCITCM6jSZMmVcsVFRVx7733xuabbz7XPikQXJAFHVOpUaNGVcspKKzpsQoBAAAAKDZdhhdir732it///vdVod2kSZPi3XffzaHhDjvsEH/4wx+q9v38888XesyilJWVRXl5+XK7FwAAAABIBIILcdlll8Wqq66au/amCUJ++MMfxoQJE/K2W265JUaPHh3t2rXL26+66qpFHrMwhxxySNx1113RpUsXk4oAAAAAsNyUVKQ+rqyUpkyZEs2aNYvyieOjrGXr2m4OAAAAALWdE5WX556oC6NCEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIIUPBCdMmBDNmzdfpnMMHjw4pk+fXvX6nHPOidtuu60aWgcAAAAA1aukoqKiIgoeCHbu3DkmT5681OcoKSmJSZMmLXOwuKSmTJkSzZo1i/KJ46OsZesavTYAAAAAK46qnKi8PMrKyha6b6EqBEeNGhU777xzdO/ePbp06RJ33333Eu3z4IMPxlZbbRWdOnXKIeKLL74Yxx13XN7Wq1evvO6zzz6LAQMGxOWXX57Xf/XVV3HEEUdE+/bt8+Pcc8+tOl/v3r3jlFNOycdusskmVecCAAAAgOWlNAoiVQAec8wx8dBDD8V6660X//3vf6Nr167xl7/8ZZH7bLvttvH111/H4YcfHk8//XS0bds2vvvuu/jmm2/immuuiSFDhsSIESPmWyF4/vnnx7fffhuvvPJKTJs2Lbbffvt8/P7775+3jx8/Pp588sl8vi233DKef/752GabbWr0vQEAAACgOAoTCD733HPx3nvvxa677jrX+rfeemux9nnjjTfiRz/6UQ7zklVWWSWXYS7K448/Hn/4wx+iXr16sdpqq8Whhx4ajz32WFUgmJ5LS0vzI1UYpoBQIAgAAADA8lKYQDANldiuXbsc+s07huCi9klSIFgd0niDc2rUqFHVcv369WPmzJnVch0AAAAAKPQYgqnb7/vvv58r9iqNHTs2ZsyYsVj79OvXLx555JF488038/rUxTcN0pg0bdq0anleu+yyS9xwww05bEzdjm+55Zbo27fvcrxTAAAAAFiwwgSCq6++ep4U5MILL8yTgqTx+k4//fSYPXv2Yu2z6aabxtChQ+OnP/1p3tazZ8+q7sYnn3xy9OnTp2pSkTn95je/yd2LO3TokI/ZY489Yr/99qvx+wcAAACApKQila6xck8nPXF8lLVsXdvNAQAAAKC2c6Ly8igrK1vovoWpEAQAAAAABIIAAAAAUCgCQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQHAFc9RRR8Xw4cNruxkAAAAA1FGltd0A5nb99dfXdhMAAAAAqMNUCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQguIxKSkpi8uTJtd0MAAAAAFgsAkEAAAAAKBCB4GJWAZ599tnRpUuX2HzzzeO2226b736nnHJKbLXVVtG5c+fYYYcd4q233qra9vzzz8f2228fnTp1io4dO8b999+f17/zzjux++675+PS+quuuqrG7gsAAACA4imt7QasTKHgSy+9FO+991507949tttuu2jVqtVc+5x22mlxySWX5OU77rgjTjzxxHj44Yfjyy+/jL322ivuueee6NWrV8yePTt3M541a1YceOCBceutt0bbtm3jm2++ia233jp69uyZA0IAAAAAqG4CwcV01FFH5efWrVvn6r+nn376e4HgY489FldeeWVMnTo1h34pCKysDmzTpk0OA5N69erFGmusEa+//nq89tprccABB1SdIx2b1gsEAQAAAFgeBILLUDE4pw8//DCOP/74GDVqVGyyySbxyiuv5OBwYSoqKnIwOHbs2OXcWgAAAAD4P8YQXExDhw7NzxMmTIgRI0ZUVftVKi8vj1VWWSXWW2+9HPTNORbgtttum8cKTMclldWDqWqwrKys6tzJu+++W1VZCAAAAADVTSC4mNJ4f2lSkb59+8YVV1zxve7CHTp0yF1/27Vrl7v7brjhhlXbVl999fjrX/8ap59+ep44pGvXrvHss89GaWlp/O1vf4v77rsvr0/HHnnkkTFt2rRauEMAAAAAiqCkIpWzscjuwZMmTYrmzZvHimTKlCnRrFmzKJ84Pspatq7t5gAAAABQ2zlReXnukbowKgQBAAAAoEBMKrIYFFECAAAAUFeoEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIALBRejevXs89dRTNXKtU045JQYPHlwj1wIAAACgmAoXCM6cObO2mwAAAAAAtabOBIIlJSVx9tlnR5cuXWLzzTeP2267ba5tgwYNiq222irOOOOM+Oyzz6J///7RoUOHaN++fQwZMqRq3+eeey46d+6c1x9++OFzBYi9e/eOYcOGVb3ed99946abbsrL5eXlcdRRR+XjOnXqFEcccURe/91338Xpp58ePXr0yOfdb7/9YtKkSXnbJ598Ev369Ystt9wydtlll/j3v/9dI+8VAAAAAMVVGnVICv5eeumleO+993JX3+222y5atWqVt9WvXz9GjRqVl/fff/9o06ZN3HfffTkc7NatWw7xunbtmrcNHTo0B3SPPvpoVeC3KAMHDoxVV101XnnllahXr158/vnnef3FF18cq622WowcOTK/Pv/883NwefXVV8cJJ5yQg8JHHnkkJk6cmAPDtm3bLrf3BwAAAADqVCCYKvSS1q1bxw477BBPP/10VSBYWbGXPP744zFmzJi8vM466+RqwbSucePGUVpamsPApG/fvvlci+Nvf/tbvPjiizkMTNZee+38nCoKU/Xgvffem1/PmDGjqk1PPPFEXHLJJXn5Bz/4Qeyxxx7V9l4AAAAAQJ0PBOdXMVipSZMmi7XfwralsHDWrFlVr6dPn77INlRUVMSVV16Zw8UlaS8AAAAALA91ZgzBJHX1TSZMmBAjRoyIXr16zXe/VAF43XXX5eXUtTd1He7Tp0/urpvGDHzyySfztlQ1OH78+KrjNt1001wFmLz//vvxzDPPVG1L1X2p2m/27NlV50322muvuOyyy+Kbb77Jr9Pza6+9VtWOG2+8sWo8weHDhy+HdwUAAAAA6miFYKreS5OKfP3113HFFVdUdc2dV9r2s5/9LE8qkir4zjrrrOjZs2feduedd8bPf/7zfK40CUkaW7DSqaeemscYTMe1a9eu6pgkhX4nnXRS3rbKKqvkY1PoeNppp8W3336b962sAEzr0vF//OMfY8CAAXlSkdRleOedd17u7xEAAAAAxVZSkRKxOiCFbWn23ubNm0dRTJkyJZo1axblE8dHWcvFG+sQAAAAgDqcE5WXR1lZWXG6DAMAAAAABekyXEcKHQEAAABguVIhCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIFgDTn44IOje/fu0bFjx9h9993j008/jQkTJkTz5s1j0KBB0a1bt9h0003joYcequ2mAgAAAFCHCQRryOWXXx6jR4+OV155JXr16hWDBw/O68vLy3NIOGbMmLjqqqvipJNOqu2mAgAAAFCHldZ2A4ri9ttvj1tuuSWmT5+eH2uttVZe36hRo+jfv39e3mabbWL8+PG13FIAAAAA6jIVgjXgmWeeiSuuuCJ3Bx43blxceumlORRMGjZsGCUlJXm5fv36MWvWrFpuLQAAAAB1mUCwBkyaNCmaNm0aa665ZsyYMSOGDBlS200CAAAAoKAEgjXgRz/6UbRp0yY/0viBnTt3ru0mAQAAAFBQJRUVFRW13QiWzpQpU6JZs2ZRPnF8lLVsXdvNAQAAAKC2c6Ly8igrK1vovioEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFIhCsYcOHD4+TTjppvtvGjRsXrVq1qvE2AQAAAFAcpbXdgKLZY4898qM6fTNjVpTOmFmt5wQAAACoS1ZdpX6UlJTUdjNWCALBZTRt2rQYMGBAvPrqq7HKKqtEixYt4uabb44DDzwwpkyZEtOnT4+ddtoprrjiiqhXr17cdNNNMWzYsPxIBg8eHLfddluUlZXFrrvuulRt6PHHsVGv4dvVfGcAAAAAdUf3jVaPu4/bRigoEFx2Dz/8cEyePDlef/31/PrLL7+Mxo0bxwMPPBBNmjSJWbNmxZ577hl33XVXHHDAAXMd++CDD8bdd98dY8aMiaZNm8YhhxxSS3cBAAAAULeN/mBSTPtuVjRuIA7zDiyjTp06xRtvvBE///nPY8cdd4zddtstZs+eHaeddlo888wzUVFREZ999lm0b9/+e4HgE088Efvtt1+uDkyOPfbYfMySGvnrbaJsrXWr7Z4AAAAA6oo01Fr3Cx6v7WasUASCy6h169a5OvAf//hHPP7443HqqafGUUcdlUPAF198MRo1ahS/+tWvctfhRVnaktXGDepLtwEAAABYLGYZXkb//ve/c5CXJgq55JJLckXgv/71r1h33XVzGPjpp5/mbsHzs8suu+RtU6dOzcdde+21Nd5+AAAAAIpFWdkySpOJnHHGGTnQmzlzZh4HMHX93XfffaNdu3bRsmXLHPzNT+pePHLkyOjatesyTSoCAAAAAIurpCIlWayU0izGzZo1i/LPP46ytdar7eYAAAAArHC+mTEztjznkbz8+nn96uywa1U5UXl51XwVC6LLMAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBBfi/vvvjy222CI6d+4cr7766gL3a9WqVYwdOzYv33TTTfHmm2/WYCsBAAAAYPEJBBfimmuuiXPOOSeHfR06dFisYwSCAAAAAKzIBIILcMIJJ8SIESPizDPPjG233TZKSkpi8uTJVdvXWmutmDBhwlzHXH/99TF69Og46aSTclXhQw89FLNmzYpf//rX0b59+/z45S9/GTNmzMj7DxgwII499tj44Q9/GJtvvnn079+/ahsAAAAA1eubGTOjoqIiik4guABXXHFFdO/ePS677LJ47rnnFuuYo446quqYVFW42267xbXXXhujRo2KMWPG5HXjx4/P2yuldQ888EC88cYb8Z///Cfuvffe5XhXAAAAAMXV/YIn4v9d83zhQ0GB4HL2+OOP50rAhg0bRmlpaRx99NHx2GOPVW3fe++9o3HjxlG/fv3o0aNHDgyXWIPVqrfRAAAAAHXU6A8mxbTvZkWRCQQXUwrsUvffStOnT1+q86Sux3Nq1KjRXNeYOXPmMrQSAAAAABZOILiYNt1003jxxRfz8n333Rdff/31fPcrKyuL8vLyqte77LJL3HzzzXlswBT2pXEG+/btW2PtBgAAAIA5CQQXUxr378QTT4yuXbvGSy+9FGuuueZ89zvmmGPiwgsvrJpUJL1Ox6RHWteqVasYOHBgjbcfAAAAAJKSiqKPorgSmzJlSjRr1ixXJKbKRAAAAAC+P7Pwluc8Mte618/rF40blEZRcyIVggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIrkDGjRsXrVq1qu1mAAAAAFCHCQSX0cyZM2u7CQAAAACw2EoXf9fiuf/+++P000+PBg0axI9+9KO44YYbYvTo0dG7d+/Yf//948knn4zNNtss/vCHP8SBBx4YU6ZMienTp8dOO+0UV1xxRdSrVy9uuummuPXWW2PttdfOFYANGzaMu+66K1q3bp2vMXjw4LjtttuirKwsdt1116Vq5zczZkbpDMEkAAAAwLy+mTGrtpuwwhEILsBnn30WRxxxRDz77LPRtm3bGDp0aHzxxRdV29Pyiy++GCUlJTkEfOCBB6JJkyYxa9as2HPPPXPod8ABB+R9R40aFWPHjo2NN944B4y///3vY8iQIfHggw/G3XffHWPGjImmTZvGIYccslRt7fE/T0S9ho2r7d4BAAAAqLt0GV6AF154ITp27JjDwOSwww7LlYKVBgwYkMPAZPbs2XHaaadFp06dokuXLrmKMAWAlbbZZpscBlYujx8/Pi8/8cQTsd9+++XqwHSuY489tobvEgAAAKAYumzQvLabsMJQIbiUUjVgpUsvvTRXFKaKwUaNGsWvfvWrXDVYKa2rVL9+/QWOO1gZMC6pkWf9MIeKAAAAAMxfRUVFtBv0aG03Y4UgEFyArbfeOl555ZV46623ok2bNnkcwBkzZsx330mTJsW6666bg79PP/00dwPeZ599FnmNXXbZJU499dQcIKaA8dprr12qtjZuUJofAAAAACx4Dgb+jxRpAdZZZ524/vrrY6+99soTgfTp0yeHds2bf7+89MQTT4x999032rVrFy1btsxB3+LYbbfdYuTIkdG1a9dlmlQEAAAAABZXSUWql2S+pk6dmif7SIYNGxZnnHFGvPHGG7GiSLMaN2vWLMrLy3UZBgAAAFhEheCW5zySl18/r1+d6225JDlR3brzanbllVfGnXfemWcOTm/kbbfdVttNAgAAAIBlIhBciDPPPDM/AAAAAKCuqFfbDQAAAAAAao5AEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEILgeDBw+O6dOn5+UBAwbE5ZdfXttNAgAAAICs9P+eqE7nnntuDBw4MBo1alQj1/tmxswonTGzRq4FAAAAsDL6Zsas2m7CCkMgWM2OO+64/NyrV6+oX79+tGzZMt5444344Q9/GB999FG0b98+7rjjjmjQoEF899138Zvf/Cb+8Y9/xIwZM2LzzTePIUOGxOqrr75E1+zxP09EvYaNl9MdAQAAAFCX6DJcza655pr8PGLEiBg7dmyss846+fmBBx7IweB//vOfuPfee/M+F198cay22moxcuTIvE+HDh3i7LPPruU7AAAAAKi7um+0eqy6Sv0oMhWCNWDvvfeOxo3/r4KvR48eMX78+Lw8bNiwKC8vrwoIU5Vgq1atlvj8I8/6YZSVlVVzqwEAAADqnlVXqR8lJSVRZALBGjDnWIKpG/HMmf833l9FRUVceeWV0bdv32U6f+MGpfkBAAAAAIuiy/By0LRp01z5tyh77bVXXHbZZfHNN9/k1+n5tddeq4EWAgAAAFBUysqWg5NPPjn69OmTuwmnSUUW5LTTTotvv/02evbsWVWqmta1a9euBlsLAAAAQJGUVKR+q6yUpkyZEs2aNcvViMYQBAAAACiuKUuQE+kyDAAAAAAFIhAEAAAAgAIRCAIAAABAgQgEAQAAAKBABIIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQgCAAAAAAFUlrbDWDpVVRU5OcpU6bUdlMAAAAAqEWV+VBlXrQwAsGV2BdffJGfN9hgg9puCgAAAAArgKlTp0azZs0Wuo9AcCW2xhpr5OcPP/xwkT9oKNq/iqSg/KOPPoqysrLabg6sUHw/YP58N2DBfD9gwXw/WJGkysAUBrZs2XKR+woEV2L16v3fEJApDPQfHvi+9L3w3YD58/2A+fPdgAXz/YAF8/1gRbG4BWMmFQEAAACAAhEIAgAAAECBCARXYg0bNoxBgwblZ+D/57sBC+b7AfPnuwEL5vsBC+b7wcqqpGJx5iIGAAAAAOoEFYIAAAAAUCACQQAAAAAoEIEgAAAAABSIQBAAAAAACkQguIK7+uqro1WrVtGoUaPo2bNnjBw5cqH733333dG2bdu8f4cOHeKhhx6qsbbCivrduOmmm6KkpGSuRzoO6pqnn346fvKTn0TLli3z53zYsGGLPOapp56Krl275pnxNt100/x9gbpoSb8f6bsx7++O9Pj0009rrM1QE37729/GVlttFU2bNo111lkn9tprr3jrrbcWeZy/OyiCpfl++NuDlYVAcAV25513xq9+9as8hfm//vWv6NSpU/Tr1y8+++yz+e7/3HPPxYEHHhhHHnlkvPTSS/k/Vukxbty4Gm87rEjfjaSsrCw++eSTqscHH3xQo22GmvD111/n70MKzBfH+++/H7vvvnvstNNOMXbs2Bg4cGAcddRR8cgjjyz3tsKK/v2olP7wm/P3R/qDEOqSf/7zn/GLX/wiXnjhhXjsscfiu+++i759++bvzIL4u4OiWJrvR+JvD1YGJRUVFRW13QjmL1U9pX+NuOqqq/Lr2bNnxwYbbBC//OUv4/TTT//e/vvvv3/+D9Pf/va3qnVbb711dO7cOa655poabTusSN+N9K90KeiYPHlyLbQWakf61+i//vWv+Q+0BTnttNPiwQcfnOsPuAMOOCB/Vx5++OEaaimsmN+PVCGYwvJJkyZF8+bNa7R9UJs+//zzHHynIGSHHXaY7z7+7qCoFuf74W8PVhYqBFdQM2bMiDFjxsQuu+xSta5evXr59fPPPz/fY9L6OfdPUtXUgvaHonw3kq+++io22mijHBzuueee8dprr9VQi2HF5fcGLFoKONZbb73o06dPPPvss7XdHFjuysvL8/Maa6yxwH38/qCoFuf7kfjbg5WBQHAF9d///jdmzZoVLVq0mGt9er2gsWvS+iXZH4ry3WjTpk3ceOONcf/998ett96aKwq33Xbb+Pe//11DrYYV04J+b0yZMiWmTZtWa+2CFUEKAVOl07333psf6Y+63r1756EqoK5K/4+UKpu22267aN++/QL383cHRbS43w9/e7CyKK3tBgAsb9tss01+VEq/kLfYYosYMmRInH/++bXaNgBWTOkPuvSY83fH+PHj47LLLotbbrmlVtsGy0saKy0NI/HMM8/UdlNgpf1++NuDlYUKwRXUWmutFfXr14///Oc/c61Pr9ddd935HpPWL8n+UJTvxrxWWWWV6NKlS7z77rvLqZWwcljQ7400EPaqq65aa+2CFVWPHj387qDOOv744/OYgE8++WSsv/76C93X3x0UzZJ8P+blbw9WVALBFVSDBg2iW7du8cQTT1StS6XG6fWc/9owp7R+zv2TNBPSgvaHonw35pW6HL/66qu5OxgUmd8bsGTSbNx+d1DXpDkmU9iRJtr5xz/+ERtvvPEij/H7g6JYmu/HvPztwYpKl+EV2K9+9as47LDDonv37vlfpC+//PI8m9fhhx+etx966KHxgx/8IH7729/m1yeeeGLsuOOO8Yc//CF23333uOOOO2L06NFx7bXX1vKdQO1+N84777w8892mm26aZ/u6+OKL44MPPoijjjqqlu8EqlcawHrOf31+//33c4CRBr7ecMMN44wzzoiJEyfGzTffnLcfd9xxebbuU089NY444oj8P7p33XVXnnkYiv79SL9b0h9+7dq1i+nTp8f111+fvyOPPvpoLd4FLJ9ukLfffnse76xp06ZV4wA2a9asqlrc3x0U1dJ8P/ztwcpCILgC23///fO05uecc07+D0+a5e7hhx+uGsD3ww8/zLOrzjk2QfqP1dlnnx1nnnlmbLbZZjFs2LCFDngKRfhuTJo0KY4++ui87+qrr54rDJ977rnYcssta/EuoPqlP8Z22mmnucLzJAXoN910U3zyySf5+1EphR0p/DvppJPij3/8Y+4Ck0KPNFMkFP37kWa1P/nkk3NI2Lhx4+jYsWM8/vjjc50D6oI//elP+TlNmjOnoUOHxoABA/KyvzsoqqX5fvjbg5VFSUWqgQUAAAAACsEYggAAAABQIAJBAAAAACgQgSAAAAAAFIhAEAAAAAAKRCAIAAAAAAUiEAQAAACAAhEIAgAAAECBCAQBAAAAoEAEggAArBB69+4dAwcOXGHOAwBQVwkEAQCIa665Jpo2bRozZ86sWvfVV1/FKquskgO2OT311FNRUlIS48ePr/F2zpgxIy666KLo1KlTNG7cONZaa63YbrvtYujQofHdd98tl2umex02bNhyOTcAQG0orZWrAgCwQtlpp51yADh69OjYeuut87oRI0bEuuuuGy+++GJMnz49GjVqlNc/+eSTseGGG8Ymm2yyxNepqKiIWbNmRWlp6VKFgf369YuXX345zj///BwElpWVxQsvvBCXXHJJdOnSJTp37hwrqhRYpoAVAKC2qRAEACDatGkT6623Xq7+q5SW99xzz9h4441z6Dbn+hQgJt9++22ccMIJsc466+TAcPvtt49Ro0bNtW+qsPv73/8e3bp1i4YNG8YzzzwTX3/9dRx66KHRpEmTfN0//OEPi2zj5ZdfHk8//XQ88cQT8Ytf/CKHf61bt46DDjooh5abbbbZYlf4NW/ePG666aaqoPH444/P7Uj3sNFGG8Vvf/vbvK1Vq1b5ee+9987nqXyd3H///dG1a9d8TGrHueeeO1eFZdr/T3/6U+yxxx6x2mqrxf/8z//EpEmT4uCDD4611147Vl111dzmVN0IAFCTBIIAAGQp5EvVf5XScuouvOOOO1atnzZtWg7fKgPBU089Ne69997485//HP/6179i0003zVV8X3755VznPv300+N3v/tdvPHGG9GxY8f49a9/Hf/85z9zqPboo4/m4DAdvzC33XZb7LLLLrkScF6p8i6FbkvjiiuuiOHDh8ddd90Vb731Vr5OZfBXGW6m0O6TTz6pep2qJ1OgeeKJJ8brr78eQ4YMyQFjCv3mNHjw4Bwmvvrqq3HEEUfEb37zm7x/CkjTe5ECw9TtGQCgJukyDABAlkK+NBlHqnJLwd9LL72Uw8DU1TWNMZg8//zzuSow7Zuq/FKglYKwXXfdNW+/7rrr4rHHHosbbrghh36VzjvvvOjTp09eTl2T0/Zbb701fvjDH+Z1KVBcf/31F9q+d95553vjGVaHDz/8MFfqperGVNWXKgQrpUq+yorC1H26UqoGTCHnYYcdll+nCsHUjTkFpIMGDaraL1UvHn744XNdKwWa3bt3z6/nrDgEAKgpKgQBAMhS2JZCvlQFlyrgNt988xyIpVCwchzBVMmXwq80hmCaVCSFhWksvzkr9Xr06JGr3+ZUGYAl6bjUTbdnz55V69ZYY43cbXlR4w8uDwMGDIixY8fm66fuz6licVHSOIYp5ExdnisfRx99dK4i/Oabb+Z738nPfvazuOOOO3J35xQePvfcc8vlngAAFkaFIAAAWerum6r0UvfgNNZdCgKTli1bxgYbbJDDq7Rt5513XuJzL2133jmlgPLNN99c4uNS1d+8YeKcMxKncQDff//93I338ccfj/322y93Tb7nnnsWeM5U5ZiqBPv37/+9bZWTr8zvvlMl5QcffBAPPfRQrqRMFZJpPMQ0KQoAQE1RIQgAQJXUFThVAabHnN1zd9hhhxyYjRw5smr8wDTLcIMGDeLZZ5+dK2hLFYZbbrnlAq+RjkuVhKnqsFIKIN9+++2Fti11v02BXerKPK903VTdOD+pyjFV7s3Z9XjOKr4kzVa8//775y7Pd955Zx4XsXIcxNTWNDPynFKImMYbTCHqvI969Rb+v9ipPamrceoynSZKufbaaxe6PwBAdVMhCABAlRT2pYq1FLBVVggmaTnNxJu6+lYGgqn6LXWBTWMFpi6/qRvxRRddlMO2I488coHXSN1r0/Z03JprrplnKD7rrLMWGaSl8Q0ffPDBXFWXxutLY/41bdo0Ro8eHb///e/zuISpK+68UkXjVVddFdtss00O9k477bQc8lW69NJL8wzDaWy/1Ia77747jxeYxg2sHOcvzWycukanWZJXX331OOecc+LHP/5xvud99903H5e6EY8bNy4uuOCCBd5DOi7NttyuXbs8FuPf/va32GKLLRbxUwEAqF4CQQAAqqSwL00o0rZt22jRosVcgeDUqVPzOHspPKuUZg6ePXt2HHLIIXl7GjPvkUceyaHZwlx88cW52+1PfvKTHOqdfPLJUV5evtBjUhiXutledtlleVbfU045JRo3bpwDtTT2X/v27ed73B/+8Ic8sUevXr1y9+c//vGPMWbMmKrt6fopyEyVg/Xr14+tttoqd+mtDCjT8b/61a9y9eAPfvCDmDBhQp5JOYV5aRzBFEamgDG9Z0cdddRC7yFVVJ5xxhn5HKuuumpuUxpTEACgJpVULK/RmQEAAACAFY4xBAEAAACgQASCAAAAAFAgAkEAAAAAKBCBIAAAAAAUiEAQAAAAAApEIAgAAAAABSIQBAAAAIACEQgCAAAAQIEIBAEAAACgQASCAAAAAFAgAkEAAAAAiOL4/wCbJiLK8aowEwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This code should be able to visualize brown clustering, however you need to experiment it with less words as it does many\n",
    "# computations for the linkage matrix.\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# Create a linkage matrix for hierarchical clustering\n",
    "linkage_matrix = linkage(co_occurrence_matrix, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(15, 10))\n",
    "dendrogram(linkage_matrix, labels=list(vocab), orientation='right', leaf_font_size=8)\n",
    "plt.xlabel('Word Clusters')\n",
    "plt.ylabel('Words')\n",
    "plt.title('Brown Clustering Dendrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üå≥ Visualizing Brown Clustering with a Dendrogram\n",
    "\n",
    "#### üìå Code Recap\n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "linkage_matrix = linkage(co_occurrence_matrix, method='ward')\n",
    "...\n",
    "dendrogram(linkage_matrix, labels=list(vocab), orientation='right')\n",
    "```\n",
    "\n",
    "This code performs **hierarchical clustering** on the **co-occurrence matrix** and displays it as a **dendrogram**.\n",
    "\n",
    "<br/>\n",
    "\n",
    "##### üå± What Is a Dendrogram?\n",
    "\n",
    "A **dendrogram** is a **tree-like diagram** that shows how words are grouped into clusters based on their co-occurrence patterns.\n",
    "\n",
    "* Each **leaf node** (at the left) is a word from the vocabulary.\n",
    "* Words that **frequently co-occur** are joined into **branches**.\n",
    "* The **horizontal axis** shows the **distance** (or dissimilarity) between merged clusters.\n",
    "\n",
    "  * **Shorter horizontal lines** ‚Üí more similar words\n",
    "  * **Longer lines** ‚Üí more dissimilar\n",
    "\n",
    "<br/>\n",
    "\n",
    "### üß† What This Means\n",
    "\n",
    "* The dendrogram shows how the words are **grouped step-by-step**.\n",
    "* It begins by clustering the **most similar pairs of words** (based on co-occurrence).\n",
    "* Then it merges those groups into larger clusters.\n",
    "\n",
    "For example:\n",
    "\n",
    "* `\"jury\"` and `\"election\"` might co-occur often ‚Üí they appear close together.\n",
    "* `\"the\"` and `\"of\"` may appear often but with many different words, making them less specific.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### ‚ö†Ô∏è Why Fewer Words?\n",
    "\n",
    "> The message in the code mentions:\n",
    "> \"you need to experiment it with less words...\"\n",
    "\n",
    "* That‚Äôs because **hierarchical clustering** requires computing pairwise distances.\n",
    "* Large vocabularies make the **linkage matrix** large and slow to process.\n",
    "* A **small subset** (e.g., 20‚Äì30 words) gives a faster and more readable result.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### üìä Summary Table\n",
    "\n",
    "| Component           | Role                                           |\n",
    "| ------------------- | ---------------------------------------------- |\n",
    "| `linkage()`         | Computes distances for hierarchical clustering |\n",
    "| `dendrogram()`      | Draws the tree structure of word clusters      |\n",
    "| X-axis (horizontal) | Dissimilarity between word clusters            |\n",
    "| Y-axis (vertical)   | Words from the vocabulary                      |\n",
    "| Short branches      | Words with similar co-occurrence contexts      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß† Introduction to GloVe (Global Vectors for Word Representation)\n",
    "\n",
    "#### **What is GloVe?**\n",
    "\n",
    "**Global Vectors for Word Representation (GloVe)** is an unsupervised learning algorithm used to learn word embeddings from large amounts of text data. Word embeddings are dense vector representations of words that capture semantic relationships between words based on their co-occurrence statistics. \n",
    "\n",
    "**GloVe** is a word embedding model that learns **vector representations of words** by using **global word co-occurrence statistics** from a corpus.\n",
    "\n",
    "\n",
    "Unlike Word2Vec (which learns embeddings through local context windows), GloVe builds a **co-occurrence matrix** and then learns word vectors from it.\n",
    "\n",
    "#### **Who developed it and when?**\n",
    "\n",
    "GloVe was developed by researchers at **Stanford University**:\n",
    "\n",
    "* **Jeffrey Pennington**, **Richard Socher**, and **Christopher Manning**\n",
    "\n",
    "It was first introduced in **2014**.\n",
    "\n",
    "#### **Who currently maintains and supports new releases?**\n",
    "\n",
    "The official code and pretrained models are available at:\n",
    "\n",
    "* [https://nlp.stanford.edu/projects/glove](https://nlp.stanford.edu/projects/glove)\n",
    "\n",
    "GloVe is not actively developed anymore, but the pretrained models are widely used.\n",
    "Unofficial implementations exist in:\n",
    "\n",
    "* `Gensim`\n",
    "* `glove-python` (Python wrapper around original C code)\n",
    "\n",
    "#### **How do I use GloVe in Python and Jupyter Notebooks?**\n",
    "\n",
    "There are two common ways:\n",
    "\n",
    "#### üì¶ Option 1: Create a model\n",
    "\n",
    "#### **What is a model in GloVe?**\n",
    "\n",
    "A **GloVe model** is a file that contains:\n",
    "\n",
    "* A large vocabulary of words\n",
    "* For each word: a **fixed-length vector** (e.g., 50D, 100D, 300D)\n",
    "\n",
    "These vectors are learned by factorizing a word-word **co-occurrence matrix** so that **similar words** have **similar vectors**.\n",
    "\n",
    "### **How to Create a GloVe Model Using Python**\n",
    "\n",
    "> GloVe training is more complex than Word2Vec and originally written in C. But here‚Äôs a basic pipeline using `glove-python-binary`:\n",
    "\n",
    "#### ‚úÖ Step 1: Install the Library\n",
    "\n",
    "```bash\n",
    "!pip install glove-python-binary\n",
    "```\n",
    "\n",
    "#### ‚úÖ Step 2: Create and Train a GloVe Model\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "#### üì¶ Option 2: Load Pretrained GloVe using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_14824\\3967532952.py:10: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2879   -0.14538  -0.016613  1.1387    1.1317    0.20705  -0.47351\n",
      "  0.15898   0.67879   0.2382    0.11621   0.26053  -0.8379    1.1232\n",
      "  0.31469  -0.47904  -0.33717  -0.34492  -0.72053  -1.0543   -0.58476\n",
      " -0.42184  -0.56977  -1.1697   -0.50389  -1.4706   -0.22005   1.1314\n",
      "  1.0829   -1.5477    0.77931  -0.22865   0.33369   0.52067   0.22109\n",
      "  1.1267    0.11704   1.1647    0.5875   -0.079243  0.022386  0.90118\n",
      " -0.37459   0.82637  -0.41061  -0.95039   0.44121  -0.58558   0.12594\n",
      " -0.71778 ]\n",
      "[('ball', 0.7129055857658386), ('gloves', 0.7020115852355957), ('throws', 0.6995697617530823), ('hat', 0.6933199167251587), ('plate', 0.6828181743621826), ('basket', 0.6727595925331116), ('tying', 0.6715709567070007), ('helmet', 0.6700937747955322), ('backboard', 0.6681182980537415), ('infield', 0.6643968820571899)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Step 1: Download and provide the pretrained glove.6B.50d.txt file somewhere accessible\n",
    "\n",
    "glove_input_file = './glove.6B/glove.6B.50d.txt'  # path to glove file\n",
    "word2vec_output_file = './glove.6B/glove.6B.50d.word2vec.txt'\n",
    "\n",
    "# Step 2: Convert glove format to word2vec format (only needs to be done once)\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "# Step 3: Load vectors\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n",
    "\n",
    "# Step 4: Use the vectors\n",
    "print(model['glove'])  # embedding vector for 'glove'\n",
    "print(model.most_similar('glove'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Steps: Preprocess the text data.<br>\n",
    "Created the dictionary.<br>\n",
    "Traverse the glove file of a specific dimension and compare each word with all words in the dictionary,\n",
    "if a match occurs, copy the equivalent vector from the glove and paste into embedding_matrix at the corresponding index.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense vector for first word is =>  [ 3.19730014e-01 -5.35109997e-01  1.68369994e-01 -5.14930010e-01\n",
      " -5.41679978e-01 -5.23229986e-02 -3.46700013e-01  4.11410004e-01\n",
      "  4.88279998e-01  1.92770004e-01 -4.34570014e-01 -5.49929999e-02\n",
      " -7.08180010e-01 -2.10759997e-01  3.05170000e-01  6.35600016e-02\n",
      "  3.52970004e-01 -1.24119997e-01 -1.21770002e-01 -4.13810015e-01\n",
      "  7.48009980e-01  2.17099994e-01  7.79249985e-03  5.02309978e-01\n",
      "  2.45900005e-01 -2.25160003e+00  5.83440006e-01  1.20770000e-01\n",
      "  1.28250003e+00 -1.01289999e+00  2.92379999e+00  7.58700013e-01\n",
      " -1.00510001e+00 -1.71330005e-01 -6.17370009e-01 -1.54649997e-02\n",
      "  3.06809992e-01 -7.09800005e-01 -3.91360015e-01  5.38070023e-01\n",
      " -1.54200003e-01  8.30769986e-02 -2.05229991e-03 -1.14189997e-01\n",
      "  6.69780016e-01 -2.23670006e-01 -1.65309995e-01 -2.11389996e-02\n",
      "  6.14579991e-02 -1.07730001e-01]\n"
     ]
    }
   ],
   "source": [
    "#Download Glove Pretrained Embeddings From: http://nlp.stanford.edu/data/glove.6B.zip  \n",
    "\n",
    "def embedding_for_vocab(filepath, word_index,\n",
    "                        embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "      \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size,\n",
    "                                       embedding_dim))\n",
    "  \n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index.index(word)\n",
    "                embedding_matrix_vocab[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "  \n",
    "    return embedding_matrix_vocab\n",
    "  \n",
    "  \n",
    "# matrix for vocab: tokenized_words\n",
    "embedding_dim = 50\n",
    "embedding_matrix_vocab = embedding_for_vocab(\n",
    "    './glove.6B/glove.6B.50d.txt', tokenized_words,\n",
    "  embedding_dim)\n",
    "  \n",
    "print(\"Dense vector for first word is => \",\n",
    "      embedding_matrix_vocab[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Comparing Wod2Vec against GloVe\n",
    "\n",
    "| **Aspect**                    | **Word2Vec**                                                                 | **GloVe**                                                                 |\n",
    "|------------------------------|------------------------------------------------------------------------------|---------------------------------------------------------------------------|\n",
    "| **Model Type**               | Predictive (learns by predicting context words)                             | Count-based (learns from global word co-occurrence)                      |\n",
    "| **Training Approach**        | Neural network with CBOW or Skip-gram                                       | Matrix factorization of word co-occurrence matrix                        |\n",
    "| **Contextual Focus**         | Local context window                                                        | Global co-occurrence statistics                                          |\n",
    "| **Use Case Suitability**     | Better at capturing semantic similarity for specific contexts               | Stronger at preserving global statistics of the entire corpus            |\n",
    "| **Performance on Rare Words**| Performs well with Skip-gram on rare/infrequent words                      | Struggles with rare words due to lack of sufficient co-occurrence        |\n",
    "| **Corpus Dependency**        | Effective on domain-specific corpora (e.g., support docs, internal KB)     | Performs best on large general corpora                                   |\n",
    "| **Training Time**            | Faster training, lower memory usage                                        | Slower training, higher memory usage due to large co-occurrence matrix   |\n",
    "| **Embedding Quality**        | Learns contextual relationships and semantic proximity                     | Captures global structure and linear relationships (e.g., analogies)     |\n",
    "| **Pretrained Availability**  | Domain-tuned or task-specific embeddings available                         | Pretrained vectors widely available (Common Crawl, Wikipedia, etc.)      |\n",
    "| **Best For**                 | Tasks like document clustering, query expansion, or contextual similarity  | Tasks requiring semantic analogy and rich word associations              |\n",
    "| **Interpretability**         | Less interpretable (weights from neural net)                               | More interpretable due to co-occurrence logic                            |\n",
    "\n",
    "### üîç Use Case Note:\n",
    "- **Word2Vec** is ideal when the corpus is **domain-specific** and we want to emphasize **contextual meaning** (e.g., chatbot knowledge base, policy docs).\n",
    "- **GloVe** works better when the corpus is **large and generic**, and we want to capture **broader semantic relationships**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction --> SVD (LSA)\n",
    "    Latent Semantic Analysis (LSA) is a technique used in natural language processing to uncover the latent structure in a corpus of text documents by applying Singular Value Decomposition (SVD) to a term-document matrix. It allows us to reduce the dimensionality of the document-term space, thereby capturing the underlying semantic relationships between words and documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [\n",
    "#     \"The quick brown fox jumps over the lazy dog.\",\n",
    "#     \"The dog barked at the fox.\",\n",
    "#     \"The fox ran away quickly.\",\n",
    "#     \"The dog is lazy.\",\n",
    "#     \"The fox is cunning.\",\n",
    "# ]\n",
    "\n",
    "# Read and parse the corpus.txt into a list of documents\n",
    "with open(\"./Data/corpus.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# Split into documents using the separator line\n",
    "corpus = raw_text.split(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "# Clean and strip empty or whitespace-only entries\n",
    "corpus = [doc.strip() for doc in corpus if doc.strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Document-Term Matrix: We use the CountVectorizer from scikit-learn to convert the text documents into a document-term matrix. Each row in the matrix corresponds to a document, and each column represents a word's frequency in that document.\n",
    "\n",
    "- Apply LSA (SVD): We use the TruncatedSVD class from scikit-learn to perform Latent Semantic Analysis. We specify the number of components (dimensions) we want to reduce the feature space to (in this case, we use n_components=2 for simplicity).\n",
    "\n",
    "- Normalize Data: To ensure that each row in the transformed matrix has unit norm, we use the Normalizer from scikit-learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 364)\t1\n",
      "  (0, 338)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 149)\t1\n",
      "  (0, 54)\t1\n",
      "  (0, 391)\t1\n",
      "  (0, 67)\t1\n",
      "  (0, 59)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 243)\t1\n",
      "  (0, 355)\t2\n",
      "  (0, 83)\t1\n",
      "  (0, 294)\t1\n",
      "  (0, 228)\t1\n",
      "  (0, 48)\t1\n",
      "  (0, 378)\t1\n",
      "  (0, 286)\t1\n",
      "  (0, 64)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 239)\t1\n",
      "  (0, 63)\t1\n",
      "  (0, 53)\t1\n",
      "  (0, 131)\t1\n",
      "  (0, 366)\t2\n",
      "  (0, 326)\t1\n",
      "  :\t:\n",
      "  (13, 290)\t1\n",
      "  (13, 369)\t1\n",
      "  (13, 156)\t1\n",
      "  (13, 190)\t1\n",
      "  (13, 126)\t1\n",
      "  (13, 399)\t1\n",
      "  (13, 351)\t1\n",
      "  (13, 45)\t1\n",
      "  (13, 12)\t1\n",
      "  (13, 166)\t1\n",
      "  (13, 4)\t1\n",
      "  (13, 308)\t1\n",
      "  (13, 320)\t1\n",
      "  (13, 75)\t1\n",
      "  (13, 282)\t1\n",
      "  (13, 299)\t1\n",
      "  (13, 271)\t1\n",
      "  (13, 345)\t1\n",
      "  (13, 260)\t1\n",
      "  (13, 265)\t1\n",
      "  (13, 185)\t1\n",
      "  (13, 237)\t1\n",
      "  (13, 304)\t1\n",
      "  (13, 37)\t1\n",
      "  (13, 121)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.50243144, -2.19150421],\n",
       "       [ 4.11174542, -2.29185878],\n",
       "       [ 4.36258273,  0.54296632],\n",
       "       [ 4.55549065,  0.28956464],\n",
       "       [ 4.58533163,  1.10445226],\n",
       "       [ 4.5201104 ,  0.56109942],\n",
       "       [ 3.96311558, -0.35640785],\n",
       "       [ 4.76513342,  2.42529461],\n",
       "       [ 4.04727581, -1.62886633],\n",
       "       [ 4.51513521, -4.99482953],\n",
       "       [ 4.29038762,  0.86676868],\n",
       "       [ 4.81365939,  2.10390625],\n",
       "       [ 4.38439361,  0.9778536 ],\n",
       "       [ 4.59351022,  1.93080225]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply SVD (Latent Semantic Analysis)\n",
    "n_components = 2  # Number of components after reducing dimensions\n",
    "lsa = TruncatedSVD(n_components)\n",
    "X_lsa = lsa.fit_transform(X)\n",
    "X_lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSA Reduced Dimensionality:\n",
      "[[ 0.89851252 -0.4389479 ]\n",
      " [ 0.8700604  -0.49294513]\n",
      " [ 0.99305129  0.11768232]\n",
      " [ 0.99780803  0.06617503]\n",
      " [ 0.97163491  0.23648593]\n",
      " [ 0.99275044  0.12019385]\n",
      " [ 0.99656743 -0.08278497]\n",
      " [ 0.89274381  0.45056463]\n",
      " [ 0.9301879  -0.36708374]\n",
      " [ 0.67076193 -0.74167273]\n",
      " [ 0.98060988  0.19597006]\n",
      " [ 0.91675801  0.39944305]\n",
      " [ 0.97429248  0.22528684]\n",
      " [ 0.92154116  0.38828068]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the transformed data\n",
    "lsa_pipeline = make_pipeline(lsa, Normalizer(copy=False))\n",
    "X_lsa_normalized = lsa_pipeline.fit_transform(X)\n",
    "print(\"\\nLSA Reduced Dimensionality:\")\n",
    "print(X_lsa_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One team member must push the final notebook to GitHub and send the `.git` URL to the instructor before the end of class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Learning Objectives\n",
    "- Implement **Word2Vec**  and **GloVe** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## üß© Workshop Structure (90 Minutes)\n",
    "1. **Instructor-led demo of predictive and count-based models** *(20 min)* ‚Äì Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(65 min)* ‚Äì NLP Pipeline and four Probabilistic Language Model method implementations + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(5 min)* ‚Äì Teams commit and push the one notebook. **Make sure to include your names so it is easy to identify the team that developed the code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(1 min)* ‚Äì Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Embeddings, Clustering, and Vectorization Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## üíª Submission Checklist\n",
    "- ‚úÖ `EmbeddingClusteringVectorizationWorkshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline on a relevant corpus (i.e., for your final project).\n",
    "  - Demo code: Implement a Word2Vec predictive model using the knowledge corpus.\n",
    "  - Demo code: Implement a GloVe count-based model using the knowledge corpus.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** in a table that compares **Word2Vec** against **GloVe** in the context of the use case that makes use of the knowledge corpus.\n",
    "- ‚úÖ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ‚úÖ GitHub Repo:\n",
    "  - Public repo named `EmbeddingClusteringVectorizationWorkshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
