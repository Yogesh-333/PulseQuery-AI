{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44ca032",
   "metadata": {},
   "source": [
    "# üß† Workshop: Building Blocks for AI Agents\n",
    "\n",
    "## NLP Pipeline + Probabilistic Language Models (90-Minute Team Lab)\n",
    "\n",
    "**Objective:**\n",
    "Work in teams of 3 to build a small NLP pipeline and implement unigram and bigram models, culminating in estimating sentence probabilities. Submit your completed Jupyter Notebook via a GitHub link (with `.git` at the end)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111a6f0",
   "metadata": {},
   "source": [
    "## Part 1 ‚Äì NLP Pipeline\n",
    "\n",
    "### Step 1: Select and Load a Corpus\n",
    "\n",
    "Select a corpus from `nltk`, or upload your own text documents. Ensure your vocabulary size exceeds 2000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b618c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Eespinosa\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "corpus_docs = [reuters.raw(fileid) for fileid in reuters.fileids()]\n",
    "corpus_text = \" \".join(corpus_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a49c6c",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** This corpus is pre-tokenized and covers multiple topics. It‚Äôs a good fit to get us above the 2,000-word vocabulary requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb1158",
   "metadata": {},
   "source": [
    "### Step 2: Collect and Preprocess Documents\n",
    "\n",
    "Convert your corpus into tokens and compute the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc40f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "tokens = word_tokenize(corpus_text.lower())\n",
    "vocab = set(tokens)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "227eccc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'punkt' tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to c:\\StudentWork\\Code\\PROG8245\\\n",
      "[nltk_data]     ProbabilisticLanguageModels\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'punkt_tab' tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to c:\\StudentWork\\Code\\PROG8\n",
      "[nltk_data]     245\\ProbabilisticLanguageModels\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Data Paths: ['C:\\\\Users\\\\Eespinosa/nltk_data', 'c:\\\\StudentWork\\\\Code\\\\PROG8245\\\\ProbabilisticLanguageModels\\\\.venv\\\\nltk_data', 'c:\\\\StudentWork\\\\Code\\\\PROG8245\\\\ProbabilisticLanguageModels\\\\.venv\\\\share\\\\nltk_data', 'c:\\\\StudentWork\\\\Code\\\\PROG8245\\\\ProbabilisticLanguageModels\\\\.venv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Eespinosa\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'c:\\\\StudentWork\\\\Code\\\\PROG8245\\\\ProbabilisticLanguageModels\\\\nltk_data']\n",
      "Contents of nltk_data: ['tokenizers']\n",
      "Vocabulary size: 52440\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Ensure 'punkt' is available and nltk_data path is set\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "print(\"Downloading 'punkt' tokenizer...\")\n",
    "nltk.download('punkt', download_dir=nltk_data_path, force=True)\n",
    "print(\"Downloading 'punkt_tab' tokenizer...\")\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_path, force=True)\n",
    "\n",
    "# Always append the custom nltk_data path (if not already present)\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Debugging paths and contents\n",
    "print(\"NLTK Data Paths:\", nltk.data.path)\n",
    "print(\"Contents of nltk_data:\", os.listdir(nltk_data_path))\n",
    "\n",
    "tokens = word_tokenize(corpus_text.lower())\n",
    "vocab = set(tokens)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1b0c8",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** Vocabulary size is important‚Äîit determines the richness of our model. Models trained on small vocabularies can't generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a46555",
   "metadata": {},
   "source": [
    "### Step 3: Implement Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7f44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "tokens = simple_tokenizer(corpus_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3cee35",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** A simple regex tokenizer gives us control‚Äîthis is useful when we need to understand every processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315d4f5",
   "metadata": {},
   "source": [
    "### Step 4: Normalization, Stemming, and Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa6a35f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eespinosa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def normalize(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "normalized_tokens = normalize(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25579af",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** Normalization makes the data more consistent and shrinks the vocabulary. This is essential for estimating reliable probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0a29b",
   "metadata": {},
   "source": [
    "## Part 2 ‚Äì Probabilistic Language Models\n",
    "\n",
    "### üìò Unigram Model\n",
    "\n",
    "A **Unigram Model** is a type of probabilistic language model that assumes each word in a sentence is **independent** of the words that came before it.\n",
    "\n",
    "The probability of a sequence of words $w_1, w_2, ..., w_n$ is calculated as:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "\n",
    "To estimate $P(w_i)$, we use the **Maximum Likelihood Estimate (MLE)**:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{\\text{count}(w_i)}{\\sum_{j} \\text{count}(w_j)}\n",
    "$$\n",
    "\n",
    "where $j$ is the total number of words in the corpus.\n",
    "\n",
    "This is a strong simplification, but it provides a foundational baseline and helps reduce data sparsity in low-resource environments.\n",
    "\n",
    "Here's how to implement it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32eb6d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('japan') = 0.00185\n",
      "P('citibank') = 0.00005\n",
      "P('harvest') = 0.00024\n",
      "P('bank') = 0.00493\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "unigram_counts = Counter(normalized_tokens)\n",
    "total_words = len(normalized_tokens)\n",
    "\n",
    "def unigram_prob(word):\n",
    "    return unigram_counts[word] / total_words\n",
    "\n",
    "print(f\"P('japan') = {unigram_prob('japan'):.5f}\")\n",
    "print(f\"P('citibank') = {unigram_prob('citibank'):.5f}\")\n",
    "print(f\"P('harvest') = {unigram_prob('harvest'):.5f}\")\n",
    "print(f\"P('bank') = {unigram_prob('bank'):.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bde8da",
   "metadata": {},
   "source": [
    "##### üìò Why Are Unigram Probabilities So Low?\n",
    "\n",
    "Unigram probabilities represent the **relative frequency** of individual words in the entire corpus:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{\\text{count}(w_i)}{\\text{total number of tokens in the corpus}}\n",
    "$$\n",
    "\n",
    "In our case, the total number of tokens is quite large:\n",
    "\n",
    "- **Total tokens:** 1,178,604  \n",
    "- **Unique words (vocabulary size):** 67,151\n",
    "\n",
    "Even if a word appears frequently, its probability will still be small relative to the total number of tokens.\n",
    "\n",
    "For example:\n",
    "- `\"bank\"` appears quite often, yet its probability is only **0.00493**, or about **0.5%** of the total words.\n",
    "- `\"citibank\"` appears only a few times, resulting in a much smaller probability of **0.00005**.\n",
    "\n",
    "These small values are expected when:\n",
    "- The corpus is **large and diverse** (like Reuters).\n",
    "- Many words appear **only once or twice**, which is common in natural language (known as Zipf's Law).\n",
    "\n",
    "**Conclusion:**  \n",
    "Low unigram probabilities do **not** indicate an error‚Äîthey reflect a realistic distribution of word frequencies across a large corpus. This also highlights the need for smoothing when building more complex language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a95505",
   "metadata": {},
   "source": [
    "### üìò Chain Rule with Unigrams\n",
    "\n",
    "Using the **Chain Rule**, we estimate the probability of a sequence:\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "This is a simplifying assumption of complete independence (unrealistic but foundational)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb0cee",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** Unigram models assume word independence‚Äîuseful but limited since word order is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12b2ab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382179640797073e-37\n"
     ]
    }
   ],
   "source": [
    "def sentence_prob_unigram(sentence):\n",
    "    words = normalize(simple_tokenizer(sentence))\n",
    "    prob = 1.0\n",
    "    for word in words:\n",
    "        prob *= unigram_prob(word)\n",
    "    return prob\n",
    "\n",
    "sentence = \"The agreement calls for joint Sino-Chilean management of the venture for 15 years, the paper said\"\n",
    "print(sentence_prob_unigram(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983954cf",
   "metadata": {},
   "source": [
    "##### üìò Why Is the Sentence Probability So Low?\n",
    "\n",
    "The calculated **unigram sentence probability** is:\n",
    "\n",
    "```python\n",
    "2.382179640797073e-37\n",
    "````\n",
    "\n",
    "This number is extremely small‚Äîbut **that‚Äôs expected** for long sentences under a unigram model. Here's why:\n",
    "\n",
    "\n",
    "##### üî¢ Corpus Statistics\n",
    "\n",
    "* **Total number of tokens:** 1,178,604\n",
    "* **Vocabulary size (unique tokens):** 67,151\n",
    "\n",
    "##### üìâ How the Unigram Model Works\n",
    "\n",
    "The unigram model computes sentence probability as the **product of individual word probabilities**:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "\n",
    "Each word typically has a probability between 0.00001 and 0.01. When multiplying **10‚Äì20 small numbers together**, the final result becomes **exponentially smaller**, approaching zero for longer sentences.\n",
    "\n",
    "##### üß™ Impact of Preprocessing (Step 4)\n",
    "\n",
    "The normalization step involves:\n",
    "\n",
    "* Lowercasing\n",
    "* **Stop word removal** (e.g., \"the\", \"of\", \"for\", \"said\")\n",
    "* **Stemming** (e.g., \"management\" ‚Üí \"manag\")\n",
    "* **Punctuation removal**\n",
    "\n",
    "This reduces the number of words used in the calculation. While this makes the vocabulary smaller and more manageable, it also means:\n",
    "\n",
    "* **Common but removed words** (like \"the\") don‚Äôt contribute to the probability.\n",
    "* **Stemmed forms** may not match original unigrams perfectly (e.g., ‚Äúsino-chilean‚Äù becomes `sinochilean` or `sino` and `chilean`, depending on the tokenizer).\n",
    "\n",
    "So even though the sentence appears long, **only 7‚Äì12 stemmed and filtered tokens** may remain after preprocessing‚Äîyet each one still has a very small individual probability.\n",
    "\n",
    "##### ‚úÖ Key Takeaways\n",
    "\n",
    "* Low sentence probabilities are **normal** in unigram models, especially for longer sentences.\n",
    "* The **multiplicative nature** of probability and the **sparsity of natural language** lead to very small final values.\n",
    "* These limitations are one reason why more advanced models (like bigrams or neural LMs) are needed for realistic NLP applications.\n",
    "\n",
    "You can inspect the intermediate tokens like this:\n",
    "\n",
    "```python\n",
    "print(normalize(simple_tokenizer(sentence)))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d7d38",
   "metadata": {},
   "source": [
    "### üìò Bigram Model with MLE ‚Äì Mathematical Explanation\n",
    "\n",
    "The **Bigram Model** assumes the current word depends only on the previous word.\n",
    "The MLE (Maximum Likelihood Estimate) for a bigram $(w_{i-1}, w_i)$ is:\n",
    "$$\n",
    "P(w_i | w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i)}{\\text{count}(w_{i-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47092802",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** This simple multiplication illustrates the chain rule, but we‚Äôll soon see how to improve this with context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d74b7c",
   "metadata": {},
   "source": [
    "### üìò Sentence Probability with Bigram Model ‚Äì Mathematical Explanation\n",
    "\n",
    "Using the bigram model and chain rule:\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_2) \\cdots P(w_n | w_{n-1})\n",
    "$$\n",
    "This models **local dependencies** between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3371af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "bigram_counts = defaultdict(int)\n",
    "\n",
    "for i in range(len(normalized_tokens) - 1):\n",
    "    pair = (normalized_tokens[i], normalized_tokens[i + 1])\n",
    "    bigram_counts[pair] += 1\n",
    "\n",
    "def bigram_prob(w1, w2):\n",
    "    return bigram_counts[(w1, w2)] / unigram_counts[w1] if unigram_counts[w1] > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a63c06",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** Bigram probabilities model word context, capturing more meaning than unigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e21066",
   "metadata": {},
   "source": [
    "### Sentence Probability with Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bacb2946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6991255325977075e-21\n"
     ]
    }
   ],
   "source": [
    "def sentence_prob_bigram(sentence):\n",
    "    words = normalize(simple_tokenizer(sentence))\n",
    "    prob = 1.0\n",
    "    for i in range(len(words) - 1):\n",
    "        prob *= bigram_prob(words[i], words[i + 1])\n",
    "    return prob\n",
    "\n",
    "sentence = \"The agreement calls for joint Sino-Chilean management of the venture for 15 years, the paper said\"\n",
    "print(sentence_prob_bigram(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447d671",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** Estimating sentence probability using bigrams shows how sequence information improves prediction power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6b238",
   "metadata": {},
   "source": [
    "## Part 3: The Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac59089",
   "metadata": {},
   "source": [
    "\n",
    "One team member must push the final notebook to GitHub and send the `.git` URL to the instructor before the end of class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7a67a",
   "metadata": {},
   "source": [
    "## üß† Learning Objectives\n",
    "- Implement the foundations of **Probabilistic Language Models** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## üß© Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(20 min)* ‚Äì Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(65 min)* ‚Äì NLP Pipeline and four Probabilistic Language Model method implementations + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(5 min)* ‚Äì Teams commit and push the one notebook. **Make sure to include your names so it is easy to identify the team that developed the code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(1 min)* ‚Äì Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Probabilistic Language Models Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## üíª Submission Checklist\n",
    "- ‚úÖ `ProbabilisticLanguageModels.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, Inverted Index and the four methods.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** (1-2 per concept)\n",
    "- ‚úÖ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ‚úÖ GitHub Repo:\n",
    "  - Public repo named `ProbabilisticLanguageModels`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b737ca3",
   "metadata": {},
   "source": [
    "## üß≠ Conclusion\n",
    "\n",
    "Today you‚Äôve constructed your own basic language model. Next class, we‚Äôll expand these ideas to explore **Large Language Models (LLMs)**‚Äîlike ChatGPT‚Äîwhich learn patterns over **massive corpora** using **deep neural networks** instead of just counts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
